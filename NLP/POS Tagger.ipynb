{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"POS Tagger.ipynb","provenance":[{"file_id":"14ZoNuFWH_gWRZuLoxAwsyjw79EJwH7IS","timestamp":1642996747105},{"file_id":"1BCzacEdP3du67ell5LX98YihA3hBNoMM","timestamp":1610735896072},{"file_id":"11NfubYI_Btu4RDJ-T_Bf9mukhj8ZzdpS","timestamp":1579635842354},{"file_id":"1BlUnw7qRED9xEkEXHHf3uSUuRD5cdT6V","timestamp":1579563004545},{"file_id":"1_gFH4E8K9LN-KxDu4XF0lOJOVXAB6YCU","timestamp":1579388129591}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gtKllKL-EFgx"},"source":["## Project 1: POS Tagger\n","\n","This notebook implements a neural part-of-speech tagger."]},{"cell_type":"code","metadata":{"id":"82H_i6foD8A_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643332116289,"user_tz":480,"elapsed":5909,"user":{"displayName":"Newton Cheng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07220749752828109188"}},"outputId":"f69c5039-2604-46c0-9dce-af5299391020"},"source":["import torch\n","\n","if torch.cuda.is_available():\n","    print('Found GPU')\n","else:\n","    print('Did not find GPU')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU\n"]}]},{"cell_type":"markdown","metadata":{"id":"SlDcpSk3VW5l"},"source":["### Part-of-Speech Tagging\n","\n","In this project, we associate each word only with its most common part of speech in the [Brown Corpus](https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html), which has been manually labeled with part-of-speech tags.  \n","\n","Words are lowercased and filtered for length and frequency. Punctuation and numbers are removed."]},{"cell_type":"code","metadata":{"id":"9T1ijH6-WlSp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643332181185,"user_tz":480,"elapsed":10133,"user":{"displayName":"Newton Cheng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07220749752828109188"}},"outputId":"a1d9fd80-83ef-4567-e0b0-8133383b0ed4"},"source":["import nltk\n","import random\n","from nltk.corpus import brown\n","from collections import defaultdict, Counter\n","\n","nltk.download('brown')\n","nltk.download('universal_tagset')\n","\n","brown_tokens = brown.tagged_words(tagset='universal')\n","print('Tagged tokens example: ', brown_tokens[:5])\n","print('Total # of word tokens:', len(brown_tokens))\n","\n","max_word_len = 20\n","\n","def most_common(s):\n","    \"Return the most common element in a sequence.\"\n","    return Counter(s).most_common(1)[0][0]\n","\n","def most_common_tags(tagged_words, min_count=3, max_len=max_word_len):\n","    \"Return a dictionary of the most common tag for each word, filtering a bit.\"\n","    counts = defaultdict(list)\n","    for w, t in tagged_words:\n","        counts[w.lower()].append(t)\n","    return {w: most_common(tags) for w, tags in counts.items() if \n","            w.isalpha() and len(w) <= max_len and len(tags) >= min_count}\n","\n","brown_types = most_common_tags(brown_tokens)\n","print('Tagged types example: ', sorted(brown_types.items())[:5])\n","print('Total # of word types:', len(brown_types))\n","\n","def split(items, test_size):\n","    \"Randomly split into train, validation, and test sets with a fixed seed.\"\n","    random.Random(288).shuffle(items)\n","    once, twice = test_size, 2 * test_size\n","    return items[:-twice], items[-twice:-once], items[-once:]\n","\n","val_test_size = 1000\n","all_data_raw = split(sorted(brown_types.items()), val_test_size)\n","train_data_raw, validation_data_raw, test_data_raw = all_data_raw\n","all_tags = sorted(set(brown_types.values()))\n","print('Tag options:', all_tags)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n","Tagged tokens example:  [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN')]\n","Total # of word tokens: 1161192\n","Tagged types example:  [('a', 'DET'), ('aaron', 'NOUN'), ('ab', 'NOUN'), ('abandon', 'VERB'), ('abandoned', 'VERB')]\n","Total # of word types: 18954\n","Tag options: ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n"]}]},{"cell_type":"markdown","metadata":{"id":"jiZ4q4aqVRVF"},"source":["We first run a baseline that predicts `NOUN` for every word, as well as setting up basic evaluation functions.\n","\n"]},{"cell_type":"code","metadata":{"id":"FvoPZ609WBpN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643332181186,"user_tz":480,"elapsed":16,"user":{"displayName":"Newton Cheng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07220749752828109188"}},"outputId":"08a95d30-a17e-4b21-9168-8fea5da6fa33"},"source":["def noun_predictor(raw_data):\n","    \"A predictor that always predicts NOUN.\"\n","    predictions = []\n","    for word, _ in raw_data:\n","        predictions.append('NOUN')\n","    return predictions\n","\n","def accuracy(predictions, targets):\n","    \"\"\"Return the accuracy percentage of a list of predictions.\n","    \n","    predictions has only the predicted tags\n","    targets has tuples of (word, tag)\n","    \"\"\"\n","    assert len(predictions) == len(targets)\n","    n_correct = 0\n","    for predicted_tag, (word, gold_tag) in zip(predictions, targets):\n","        if predicted_tag == gold_tag:\n","            n_correct += 1\n","\n","    return n_correct / len(targets) * 100.0\n","\n","def evaluate(predictor, raw_data):\n","    return accuracy(predictor(raw_data), raw_data)\n","\n","def print_sample_predictions(predictor, raw_data, k=10):\n","    \"Print the first k predictions.\"\n","    d = raw_data[:k]\n","    print('Sample predictions:', \n","          [(word, guess) for (word, _), guess in zip(d, predictor(d))])\n","\n","print('noun baseline validation accuracy:', \n","      evaluate(noun_predictor, validation_data_raw))\n","print_sample_predictions(noun_predictor, validation_data_raw)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["noun baseline validation accuracy: 55.1\n","Sample predictions: [('salem', 'NOUN'), ('unsympathetic', 'NOUN'), ('downwind', 'NOUN'), ('exodus', 'NOUN'), ('avoiding', 'NOUN'), ('informal', 'NOUN'), ('padded', 'NOUN'), ('tantalizing', 'NOUN'), ('farce', 'NOUN'), ('berger', 'NOUN')]\n"]}]},{"cell_type":"markdown","metadata":{"id":"jIxSaG7pavpS"},"source":["### Pytorch Tagger\n","We now build a neural tagging model. We need to do some basic data pre-processing by padding."]},{"cell_type":"code","metadata":{"id":"3N8wiRpoiZzG"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f62L4sTrUEn3"},"source":["def make_matrices(data_raw):\n","    \"\"\"Convert a list of (word, tag) pairs into tensors with appropriate padding.\n","    \n","    character_matrix holds character codes for each word, \n","      indexed as [word_index, character_index]\n","    character_mask masks valid characters (1 for valid, 0 invalid), \n","      indexed similarly so that all inputs can have a constant length\n","    pos_labels holds part-of-speech values for each word as integer indices\n","    \"\"\"\n","    max_len = max_word_len + 2  # leave room for word start/end symbols\n","    character_matrix = torch.zeros(len(data_raw), max_len, dtype=torch.int64) \n","    character_mask = torch.zeros(len(data_raw), max_len, dtype=torch.float32)\n","    pos_labels = torch.zeros(len(data_raw), dtype=torch.int64)\n","    for word_i, (word, pos) in enumerate(data_raw):\n","        for char_i, c in enumerate('^' + word + '$'):\n","            character_matrix[word_i, char_i]= ord(c)\n","            character_mask[word_i, char_i] = 1\n","        pos_labels[word_i] = all_tags.index(pos)\n","    return torch.utils.data.TensorDataset(character_matrix, character_mask, pos_labels)\n","\n","validation_data = make_matrices(validation_data_raw)\n","\n","print('Sample datapoint after preprocessing:', validation_data[0])\n","print('Raw datapoint:', validation_data_raw[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ubdajTtCwzt"},"source":["We define a predictor for a network, taking a list of words (strings) and returning a list of part-of-speech tags (also strings)."]},{"cell_type":"code","metadata":{"id":"zlWQk-dcp9BD"},"source":["def predict_using(network):\n","    def predictor(raw_data):\n","        \"\"\"Return a list of part-of-speech tags as strings, one for each word.\n","\n","        raw_data - a list of (word, tag) pairs.\n","        \"\"\"\n","        with torch.no_grad():\n","\n","            predictions = []\n","\n","            network.eval()\n","            processed_data = make_matrices(raw_data)\n","            loader = torch.utils.data.DataLoader(processed_data)\n","            for chars_batch, mask_batch, pos_labels in loader:\n","                chars_batch, mask_batch = chars_batch.cuda(), mask_batch.cuda()\n","                logits = network(chars_batch, mask_batch)\n","                pred = torch.argmax(logits)\n","                predictions.append(all_tags[pred])\n","\n","            network.train()\n","\n","            return predictions\n","\n","    return predictor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C_9jzlbFB1hw"},"source":["\n","Now we define a helper training function."]},{"cell_type":"code","metadata":{"id":"w7ce8kZd-5pj"},"source":["import tqdm\n","\n","def train(network, n_epochs=25):\n","    processed_data = make_matrices(train_data_raw)\n","    data_loader = torch.utils.data.DataLoader(processed_data)\n","    network = network.cuda()\n","    optimizer = torch.optim.Adam(network.parameters())\n","    \n","    predictor = predict_using(network)\n","\n","    for epoch in range(n_epochs):\n","        print('Epoch', epoch)\n","        for batch in tqdm.tqdm_notebook(data_loader, leave=False):\n","            chars_batch, mask_batch, pos_batch = batch\n","            assert network.training\n","\n","            optimizer.zero_grad()\n","            chars_batch, mask_batch, pos_batch = chars_batch.cuda(), mask_batch.cuda(), pos_batch.cuda()\n","            output = network(chars_batch, mask_batch)\n","            loss = F.cross_entropy(output, pos_batch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        validation_score = evaluate(predictor, validation_data_raw)\n","        print('Validation score:', validation_score)\n","\n","        # early stopping\n","        if (epoch == 0) or (validation_score > best_score):\n","            best_score = validation_score\n","            torch.save(network.state_dict(), 'network.pt')\n","\n","    network.load_state_dict(torch.load('network.pt'))\n","\n","    return network"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hT0Fj_de2j5"},"source":["class POSTagger(nn.Module):\n","    def __init__(self, n_outputs): # pass whatever arguments you need\n","        super().__init__()\n","\n","        # YOUR CODE HERE\n","        # create Modules from torch.nn (imported as nn)\n","\n","        # BEGIN SOLUTION\n","        self.embeddings = nn.Embedding(256, 44)\n","        self.linear0 = nn.Linear(132, 256)\n","        self.linear1 = nn.Linear(256, 256)\n","        self.linear2 = nn.Linear(256, n_outputs)\n","        # END SOLUTION\n","\n","    def forward(self, chars, mask):\n","        # for this network, `chars` should be an int64 tensor of character ids with size (batch, n_chars)\n","        # `mask` is a float32 tensor of size (batch, n_chars) that is 1.0 if the character at that position in `chars` is valid (else 0.0)\n","        # the function returns a float32 tensor of size (batch, n_pos)\n","\n","        # YOUR CODE HERE\n","\n","        # BEGIN SOLUTION\n","        embeds = self.embeddings(chars)\n","        # implementing character trigrams\n","        concat_embed = torch.cat((embeds[:,:-2], embeds[:,1:-1], embeds[:,2:]), dim=2).squeeze(0)\n","        concat_embed = F.relu(self.linear0(concat_embed)).unsqueeze(0)\n","        pooled_masked_embeds = (concat_embed*mask[:,2:].unsqueeze(-1)).mean(1)\n","        output = F.relu(self.linear1(pooled_masked_embeds))\n","        output = F.dropout(output, training=self.training)\n","        output = self.linear2(output)\n","\n","        return output\n","        # END SOLUTION\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AFMX_RBKT1oN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643336698974,"user_tz":480,"elapsed":102,"user":{"displayName":"Newton Cheng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07220749752828109188"}},"outputId":"607d81a4-3e4c-4c72-ce12-7b97fa379c83"},"source":["print_sample_predictions(improved_predictor, validation_data_raw)\n","\n","print_sample_predictions(improved_predictor, [['kleining','X'], ['deneroful','X']])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample predictions: [('salem', 'NOUN'), ('unsympathetic', 'ADJ'), ('downwind', 'VERB'), ('exodus', 'NOUN'), ('avoiding', 'VERB'), ('informal', 'ADJ'), ('padded', 'VERB'), ('tantalizing', 'VERB'), ('farce', 'NOUN'), ('berger', 'NOUN')]\n","Sample predictions: [('kleining', 'VERB'), ('deneroful', 'ADJ')]\n"]}]},{"cell_type":"markdown","metadata":{"id":"XrsWMTREe9EY"},"source":["We can save outputs."]},{"cell_type":"code","metadata":{"id":"uRxZb1LBfVmf"},"source":["def save_predictions(predictions, filename):\n","    \"\"\"Save predictions to a file.\n","    \n","    predictions is a list of strings.\n","    \"\"\"\n","    with open(filename, 'w') as f:\n","        for pred in predictions:\n","            f.write(pred)\n","            f.write('\\n')\n","\n","print('test score improved:', evaluate(improved_predictor, test_data_raw))\n","test_predictions = improved_predictor(test_data_raw)\n","save_predictions(test_predictions, 'predicted_test_outputs_improved.txt')"],"execution_count":null,"outputs":[]}]}