{"cells":[{"cell_type":"markdown","metadata":{"id":"430w1zf5CD5B"},"source":["## Project 3: Using Large Language Models\n","\n","In this project, I use the GPT-2 model via the HuggingFrace transformers API to perform autoregressive language modelling, as well as prompt-based sentiment classification."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJaHQbVKB97u"},"outputs":[],"source":["import math\n","import numpy as np\n","import random\n","import pdb\n","from collections import defaultdict, Counter\n","import matplotlib.pyplot as plt\n","import tqdm\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torchtext.legacy as torchtext\n","\n","from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n","\n","# downloading and loading WikiText2 data\n","text_field = torchtext.data.Field()\n","train_dataset, validation_dataset, test_dataset = torchtext.datasets.WikiText2.splits(root='.', text_field=text_field)\n","text_field.build_vocab(train_dataset, validation_dataset, test_dataset)\n","vocab = text_field.vocab\n","vocab_size = len(vocab)\n","train_text = train_dataset.examples[0].text # a list of tokens (strings)\n","validation_text = validation_dataset.examples[0].text"]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(device)\n","tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2-large\")"],"metadata":{"id":"MxFFNqF4DgW8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WBfwge86hzqA"},"source":["### Language Modeling with GPT-2\n","\n","In the first part, we use GPT-2 to predict word probabilities."]},{"cell_type":"markdown","metadata":{"id":"ZnIQqtT5lKfz"},"source":["Start by tokenizing the vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KrZao3FriCVW"},"outputs":[],"source":["vocab_map = {}\n","vocab = tokenizer.vocab\n","for token in tqdm.tqdm(vocab):\n","    idx = vocab[token]\n","    vocab_map[idx] = token"]},{"cell_type":"markdown","metadata":{"id":"GMHp2Y6qmTlh"},"source":["The tokenizer for GPT-2 uses subwords, so we reconstruct the words using the the word-start \"Ġ\" token. The probabilities are obtained by adding the log-probabiliities of the corresponding subword tokens.\n","\n"]},{"cell_type":"code","source":["def get_gpt_probs(sentence):\n","    '''Outputs word probabilities given an input string as a sentence\n","    \n","    sentence: string\n","    returns: list of 2-tuples with a word and its negative log-probability\n","    '''\n","    words = []\n","    probs = []\n","\n","    # append the sentence start token and tokenize the sentence\n","    sentence = '<|endoftext|>'+sentence\n","    encodings = tokenizer(sentence, return_tensors='pt').to(device)\n","    subwords = [vocab_map[int(idx)] for idx in encodings['input_ids'][0]][1:]\n","\n","    logits = model(encodings['input_ids']).logits.squeeze(0)[:-1]\n","    logit_norm = F.log_softmax(logits, dim=-1)\n","    subprobs = logit_norm.gather(-1, encodings['input_ids'][0,1:].unsqueeze(-1)).flatten().tolist()\n","    \n","    # turn tokens into words, paired with their log-probabilities\n","    current_word = subwords[0]\n","    current_prob = subprobs[0]\n","    for i, subword in enumerate(subwords[1:]):\n","        if (subword[0] != 'Ġ'):\n","            current_word += subword\n","            current_prob += subprobs[i+1]\n","        else:\n","            words.append(current_word)\n","            probs.append(current_prob)\n","            current_word = subword\n","            current_prob = subprobs[i+1]\n","\n","    if len(current_word) > 0:\n","        words.append(current_word)\n","        probs.append(current_prob)\n","\n","    # END SOLUTION\n","    return [(word, -1*prob) for word, prob in zip(words, probs)]\n","            \n"],"metadata":{"id":"hGZbcO3YFQ7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMN_3N3ui5_y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"de133f88-8fc9-421d-ea63-f36c0d65d97f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('The', 2.4790914058685303),\n"," ('Ġflorist', 13.309796899557114),\n"," ('Ġsent', 8.14240837097168),\n"," ('Ġthe', 2.366187334060669),\n"," ('Ġflowers', 1.405614972114563),\n"," ('Ġwas', 9.130631446838379),\n"," ('Ġpleased.', 9.91234540939331)]"]},"metadata":{},"execution_count":5}],"source":["get_gpt_probs(\"The florist sent the flowers was pleased.\")"]},{"cell_type":"markdown","metadata":{"id":"sKwBKt3VtSCy"},"source":["We now implement a modification to allow for efficient modeling of long text documents: using a strided window, rather than a standard 1-token sliding window. The model uses a context of 1024 and stride of 512. Only the probabilities generated for a word the first time are used in the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKeL7Jtoi9VX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"82e16681-c668-441c-a401-99657a1feac9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Context length: 1024\n"]}],"source":["stride = 512\n","print(\"Context length: {}\".format(model.config.n_positions))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WsZDbz-6yqJE"},"outputs":[],"source":["def get_gpt_probs(document, stride=512, no_tqdm=False):\n","    '''Outputs word probabilities given an input document string using 1024\n","    context and 512 stride\n","    \n","    document: string\n","    returns: list of 2-tuples with a word and its negative log-probability\n","    '''\n","    words = []\n","    probs = []\n","\n","    encodings = tokenizer('<|endoftext|>'*stride+document, return_tensors='pt').to(device)\n","    subwords = [vocab_map[int(idx)] for idx in encodings['input_ids'][0]][stride:]\n","    \n","    subprobs = []\n","    for i in tqdm.notebook.tqdm(range(0, encodings['input_ids'].shape[1], stride), disable=no_tqdm):\n","        left = max(0, i + stride - 1024)\n","        right = i + stride\n","        text = encodings['input_ids'][:,left:right].to(device)\n","        length = text.shape[1]\n","        mask = text.clone()\n","        # we don't want to consider the first 512 tokens of the window; we've already predicted their probabilities\n","        mask[:,:-stride] = -100\n","        \n","        with torch.no_grad():\n","            logits = model(text).logits.squeeze(0)\n","            logits = logits[stride-length:]\n","            logit_norm = F.log_softmax(logits, dim=-1)\n","            subprob = logit_norm.gather(-1, text[:,stride-length:].T).flatten().tolist()\n","            subprobs += subprob\n","\n","    current_word = subwords[0]\n","    current_prob = subprobs[stride-1]\n","\n","    for i, subword in enumerate(subwords[1:]):\n","        if (subword[0] != 'Ġ'):\n","            current_word += subword\n","            current_prob += subprobs[i+stride]\n","        else:\n","            words.append(current_word)\n","            probs.append(current_prob)\n","            current_word = subword\n","            current_prob = subprobs[i+stride]\n","\n","    if len(current_word) > 0:\n","        words.append(current_word)\n","        probs.append(current_prob)\n","\n","    # END SOLUTION\n","    return [(word, -1*prob) for word, prob in zip(words, probs)]"]},{"cell_type":"markdown","metadata":{"id":"i4SHhy4N5ZfT"},"source":["This function can be used to generate probabilities for long texts, such as *Wuthering Heights*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXbHUzV50Ihn"},"outputs":[],"source":["with open(\"wuthering.txt\", \"r\") as text_file:\n","  text = text_file.read()"]},{"cell_type":"code","source":["output = get_gpt_probs(text, stride=512, no_tqdm=False)\n","probs = np.asarray([val[1] for val in output])"],"metadata":{"id":"CthLzS0ParzW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8VibM4aB54IK"},"source":["### Prompting\n","\n","In this part, we experiment with adapting GPT-2 for sentiment classification via prompting. We use the task set SST-2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lc80b6BX1iR0"},"outputs":[],"source":["from datasets import load_dataset\n","dataset = load_dataset(\"glue\", \"sst2\")"]},{"cell_type":"markdown","source":["Sentiment classification is done by prompting using examples from the training dataset, and then comparing the probabilities of answering yes or no to a question on the sentiment of an input.\n","\n","We implement a thresholding function that adjusts for bias in the prompt: due to the high variance in outputs arising from the specific details of the prompt structure, we find it useful to manually correct for bias in the prompt. The correcting is determined by the probabilities obtained from a neutral input."],"metadata":{"id":"DtmhDp_PT8EC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"byloT-Sz8mQF"},"outputs":[],"source":["prompt = \"Opinion: I love this book! Positive? Yes. Opinion: I like having fun. Positive? Yes. Opinion: This food is terrible. Positive? No. Opinion: I hate you. Positive? No. Opinion: Funny and whimsical. Positive? Yes.\"\n","\n","def bias():\n","    bias_pos_input = prompt + ' Opinion: ' + 'N/A' + '. Positive? Yes.'\n","    bias_neg_input = prompt + ' Opinion: ' + 'N/A' + '. Positive? No.'\n","    bias_pos = get_gpt_probs(bias_pos_input)[-1][1]\n","    bias_neg = get_gpt_probs(bias_neg_input)[-1][1]\n","    bias = bias_pos - bias_neg\n","\n","    return bias\n","\n","def predict_sentiment(sentence, bias):\n","    pos_input = prompt + ' Opinion: ' + sentence + '. Positive? Yes.'\n","    neg_input = prompt + ' Opinion: ' + sentence + '. Positive? No.'\n","    pos_prob = get_gpt_probs(pos_input)[-1][1]\n","    neg_prob = get_gpt_probs(neg_input)[-1][1]\n","\n","    #get_gpt_probs returns negative log-likelihoods\n","    if pos_prob <= neg_prob - bias:\n","        return 1\n","    else:\n","        return 0\n"]},{"cell_type":"markdown","source":["Checking performance on the training dataset."],"metadata":{"id":"mhnkkWCUIRIz"}},{"cell_type":"code","source":["num_correct = 0\n","FP = 0\n","FN = 0\n","\n","for idx in tqdm.notebook.tqdm(range(1000)):\n","  example = dataset[\"train\"][idx]\n","  predicted_label = predict_sentiment(example[\"sentence\"], bias)\n","  if predicted_label == example[\"label\"]:\n","    num_correct += 1\n","  elif predicted_label - example[\"label\"] > 0:\n","    FP += 1\n","  else:\n","    FN += 1\n","    \n","print(f\"Accuracy: {num_correct / 1000}\")\n","print('FP: ', FP)\n","print('FN: ', FN)"],"metadata":{"id":"rr0NQwG0i5_C"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"hw1c.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}