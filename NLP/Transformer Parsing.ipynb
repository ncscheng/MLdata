{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"Copy of hw3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Project 6: Transformer Parsing"],"metadata":{"id":"CeUOYktpXtOz"}},{"cell_type":"markdown","source":["In this project, we implement a Transformer encoder and apply it to a constituency parser. It is trained to classify span labels, and we use the CKY algorithm to turn the predictions into trees.\n","\n"],"metadata":{"id":"35m1Uv47jiNX"}},{"cell_type":"code","source":["from copy import deepcopy\n","import json\n","import math\n","import random\n","import numpy as np\n","import sentencepiece\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","import tqdm.notebook\n","import matplotlib.pyplot as plt\n","import nltk\n","from nltk.corpus.reader.bracket_parse import BracketParseCorpusReader\n","import svgling\n","svgling.disable_nltk_png()"],"metadata":{"id":"4LV8KY6_unfe","execution":{"iopub.status.busy":"2022-03-07T18:02:25.333526Z","iopub.execute_input":"2022-03-07T18:02:25.333989Z","iopub.status.idle":"2022-03-07T18:02:39.801268Z","shell.execute_reply.started":"2022-03-07T18:02:25.333833Z","shell.execute_reply":"2022-03-07T18:02:39.800477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"KYpIPtqtwVwh","execution":{"iopub.status.busy":"2022-03-07T18:02:39.803325Z","iopub.execute_input":"2022-03-07T18:02:39.803594Z","iopub.status.idle":"2022-03-07T18:02:39.849609Z","shell.execute_reply.started":"2022-03-07T18:02:39.803558Z","shell.execute_reply":"2022-03-07T18:02:39.848807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data"],"metadata":{"id":"ANK-5cMtYSyH"}},{"cell_type":"markdown","source":["We use the standard Penn Treebank data splits for parsing: sections 2 to 21 for training, section 22 for validation, and section 23 for testing."],"metadata":{"id":"BZGmlRlB-2uo"}},{"cell_type":"code","source":["%%bash\n","if [ ! -e parsing-data.zip ]; then\n","  wget --quiet https://storage.googleapis.com/cs288-parsing-project/parsing-data.zip\n","fi\n","rm -rf train dev test EVALB/\n","unzip parsing-data.zip"],"metadata":{"id":"ZMYj6yJKZ2j_","execution":{"iopub.status.busy":"2022-03-07T18:02:39.850943Z","iopub.execute_input":"2022-03-07T18:02:39.851425Z","iopub.status.idle":"2022-03-07T18:02:40.289615Z","shell.execute_reply.started":"2022-03-07T18:02:39.851349Z","shell.execute_reply":"2022-03-07T18:02:40.28881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["READER = BracketParseCorpusReader('.', ['train', 'dev', 'test'])"],"metadata":{"id":"eGzu6IjkBW74","execution":{"iopub.status.busy":"2022-03-07T18:02:40.969231Z","iopub.execute_input":"2022-03-07T18:02:40.969922Z","iopub.status.idle":"2022-03-07T18:02:40.975165Z","shell.execute_reply.started":"2022-03-07T18:02:40.969864Z","shell.execute_reply":"2022-03-07T18:02:40.974399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Vocabulary"],"metadata":{"id":"k1aBbR3snOjV"}},{"cell_type":"markdown","source":["We preprocess the tree data into standard sentences that can be tokenized and input into a neural model."],"metadata":{"id":"dbVpX3RlMXBW"}},{"cell_type":"code","source":["with open('sentences.txt', 'w') as f:\n","  for sent in READER.sents('train'):\n","    f.write(' '.join(sent) + '\\n')"],"metadata":{"id":"Yasw_R3hB_13","execution":{"iopub.status.busy":"2022-03-07T18:02:40.992087Z","iopub.execute_input":"2022-03-07T18:02:40.992674Z","iopub.status.idle":"2022-03-07T18:02:45.132701Z","shell.execute_reply.started":"2022-03-07T18:02:40.992636Z","shell.execute_reply":"2022-03-07T18:02:45.1318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = {\n","    \"pad_id\": 0,\n","    \"bos_id\": 1,\n","    \"eos_id\": 2,\n","    \"unk_id\": 3,\n","    \"input\": \"sentences.txt\",\n","    \"vocab_size\": 16000,\n","    \"model_prefix\": \"ptb\",\n","}\n","combined_args = \" \".join(\n","    \"--{}={}\".format(key, value) for key, value in args.items())\n","sentencepiece.SentencePieceTrainer.Train(combined_args)"],"metadata":{"id":"8xSUaso9vo1V","execution":{"iopub.status.busy":"2022-03-07T18:02:45.808399Z","iopub.execute_input":"2022-03-07T18:02:45.808881Z","iopub.status.idle":"2022-03-07T18:02:51.509757Z","shell.execute_reply.started":"2022-03-07T18:02:45.808843Z","shell.execute_reply":"2022-03-07T18:02:51.506975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["VOCAB = sentencepiece.SentencePieceProcessor()\n","VOCAB.Load(\"ptb.model\")"],"metadata":{"id":"SJYzMQxfvrr0","execution":{"iopub.status.busy":"2022-03-07T18:02:52.222869Z","iopub.execute_input":"2022-03-07T18:02:52.223706Z","iopub.status.idle":"2022-03-07T18:02:52.260598Z","shell.execute_reply.started":"2022-03-07T18:02:52.223618Z","shell.execute_reply":"2022-03-07T18:02:52.259674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PAD_ID = VOCAB.PieceToId(\"<pad>\")\n","BOS_ID = VOCAB.PieceToId(\"<s>\")\n","EOS_ID = VOCAB.PieceToId(\"</s>\")\n","UNK_ID = VOCAB.PieceToId(\"<unk>\")"],"metadata":{"id":"xheKi30BVVJC","execution":{"iopub.status.busy":"2022-03-07T18:02:52.262356Z","iopub.execute_input":"2022-03-07T18:02:52.262632Z","iopub.status.idle":"2022-03-07T18:02:52.269531Z","shell.execute_reply.started":"2022-03-07T18:02:52.262598Z","shell.execute_reply":"2022-03-07T18:02:52.268651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part-of-Speech Tagging Setup"],"metadata":{"id":"-uzrJDCU1Tei"}},{"cell_type":"markdown","source":["We implement POS tagging as a way to create the baseline, single-word-span that can be recursively operated on to produce full constituency tagging tables.\n","\n","Sentences are encoded at the subword level, which is translated to the word level by choosing the last subword as a word representation, masking the other tokens."],"metadata":{"id":"1P3NgM0cBHnD"}},{"cell_type":"code","source":["def encode_sentence(sent):\n","  \"\"\"Subword tokenization of an input sentence.\n","\n","  Args:\n","    sent: a list of strings of words in the sentence\n","  Returns:\n","    A tuple (ids, is_word_end).\n","      ids: a list of token ids in the subword vocabulary\n","      is_word_end: a list of bools, where True indicates that the token\n","        is the last of the word\n","  \"\"\"\n","  ids = []\n","  is_word_end = []\n","  for word in sent:\n","    word_ids = VOCAB.EncodeAsIds(word)\n","    ids.extend(word_ids)\n","    is_word_end.extend([False] * (len(word_ids) - 1) + [True])\n","  return ids, is_word_end"],"metadata":{"id":"tvlk8axRvGiB","execution":{"iopub.status.busy":"2022-03-07T18:02:52.272125Z","iopub.execute_input":"2022-03-07T18:02:52.27281Z","iopub.status.idle":"2022-03-07T18:02:52.279996Z","shell.execute_reply.started":"2022-03-07T18:02:52.272773Z","shell.execute_reply":"2022-03-07T18:02:52.27919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Downloading the PennTreebank tagset for training."],"metadata":{"id":"gY26VRXtBR5J"}},{"cell_type":"code","source":["nltk.download('tagsets')\n","nltk.help.upenn_tagset()"],"metadata":{"id":"-iDS1lcXvQaN","execution":{"iopub.status.busy":"2022-03-07T18:02:52.309645Z","iopub.execute_input":"2022-03-07T18:02:52.310255Z","iopub.status.idle":"2022-03-07T18:02:52.407054Z","shell.execute_reply.started":"2022-03-07T18:02:52.310217Z","shell.execute_reply":"2022-03-07T18:02:52.406334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We get all possible POS tags in the data."],"metadata":{"id":"veJzgeJDBdw1"}},{"cell_type":"code","source":["def get_pos_vocab():\n","  all_pos = set()\n","  for sent in READER.tagged_sents('train'):\n","    for word, pos in sent:\n","      all_pos.add(pos)\n","  return sorted(all_pos)\n","\n","PARTS_OF_SPEECH = get_pos_vocab()"],"metadata":{"id":"u_QW5pk2G7-W","execution":{"iopub.status.busy":"2022-03-07T18:02:52.408244Z","iopub.execute_input":"2022-03-07T18:02:52.408468Z","iopub.status.idle":"2022-03-07T18:02:56.878125Z","shell.execute_reply.started":"2022-03-07T18:02:52.408435Z","shell.execute_reply":"2022-03-07T18:02:56.877375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We implement the `POSTaggingDataset` for input into a model.\n","\n","Each example in the dataset is a feature dictionary, consisting of word piece `ids`, and corresponding label ids `labels`. Masked subword labels (non-ending tokens) have mask label -1.\n","\n","We also define a `collate` function that pads examples for a batch."],"metadata":{"id":"ifj0Isu3B5Hj"}},{"cell_type":"code","source":["class POSTaggingDataset(torch.utils.data.Dataset):\n","  def __init__(self, split):\n","    assert split in ('train', 'dev', 'test')\n","    self.sents = READER.tagged_sents(split)\n","    if split == 'train':\n","      # To speed up training, we only train on short sentences.\n","      self.sents = [sent for sent in self.sents if len(sent) <= 40]\n","\n","  def __len__(self):\n","    return len(self.sents)\n","\n","  def __getitem__(self, index):\n","    sent = self.sents[index]\n","    ids, is_word_end = encode_sentence([word for word, pos in sent])\n","    ids = [BOS_ID] + ids + [EOS_ID]\n","    is_word_end = [False] + is_word_end + [False]\n","    ids = torch.tensor(ids)\n","    is_word_end = torch.tensor(is_word_end)\n","    labels = torch.full_like(ids, -1)\n","    labels[is_word_end] = torch.tensor(\n","        [PARTS_OF_SPEECH.index(pos) for word, pos in sent])\n","    return {'ids': ids, 'labels': labels}\n","\n","  @staticmethod\n","  def collate(batch):\n","    ids = pad_sequence(\n","        [item['ids'] for item in batch],\n","        batch_first=True, padding_value=PAD_ID)\n","    labels = pad_sequence(\n","        [item['labels'] for item in batch],\n","        batch_first=True, padding_value=-1)\n","    return {'ids': ids.to(device), 'labels': labels.to(device)}"],"metadata":{"id":"dT4iBQ1eBoSJ","execution":{"iopub.status.busy":"2022-03-07T18:02:56.87937Z","iopub.execute_input":"2022-03-07T18:02:56.879609Z","iopub.status.idle":"2022-03-07T18:02:56.891683Z","shell.execute_reply.started":"2022-03-07T18:02:56.879575Z","shell.execute_reply":"2022-03-07T18:02:56.890934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Baseline POS Tagging Model"],"metadata":{"id":"KiSPP3tdyaid"}},{"cell_type":"markdown","source":["We implement the baseline tagging model that will serve as the base for the Transformer architecture."],"metadata":{"id":"gGIIy0230Zdc"}},{"cell_type":"code","source":["class POSTaggingModel(nn.Module):\n","  def encode(self, batch):\n","    raise NotImplementedError()\n","\n","  def compute_loss(self, batch):\n","    logits = self.encode(batch)\n","    logits = logits.reshape((-1, logits.shape[-1]))\n","    labels = batch['labels'].reshape((-1,))\n","    res = F.cross_entropy(logits, labels, ignore_index=-1, reduction='mean')\n","    return res\n","  \n","  def get_validation_metric(self, batch_size=8):\n","    dataset = POSTaggingDataset('dev')\n","    data_loader = torch.utils.data.DataLoader(\n","      dataset, batch_size=batch_size, collate_fn=dataset.collate)\n","    self.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","      for batch in data_loader:\n","        mask = (batch['labels'] != -1)\n","        predicted_labels = self.encode(batch).argmax(-1)\n","        predicted_labels = predicted_labels[mask]\n","        gold_labels = batch['labels'][mask]\n","        correct += (predicted_labels == gold_labels).sum().item()\n","        total += gold_labels.shape[0]\n","    return correct / total"],"metadata":{"id":"Dz3pUko2L8Fj","execution":{"iopub.status.busy":"2022-03-07T18:03:04.516235Z","iopub.execute_input":"2022-03-07T18:03:04.516471Z","iopub.status.idle":"2022-03-07T18:03:04.526544Z","shell.execute_reply.started":"2022-03-07T18:03:04.516438Z","shell.execute_reply":"2022-03-07T18:03:04.525792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We use a warmup learning rate schedule, where the learning rate is increased linearly from 0 to its maximum value during a warm-up phase, and is then decayed as training progresses."],"metadata":{"id":"jx9jJQb58nUY"}},{"cell_type":"code","source":["def train(model, num_epochs, batch_size, model_file,\n","          learning_rate=8e-4, dataset_cls=POSTaggingDataset):\n","  \"\"\"Train the model and save the one with best validation loss.\"\"\"\n","  dataset = dataset_cls('train')\n","  data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=batch_size, shuffle=True, collate_fn=dataset.collate)\n","  optimizer = torch.optim.Adam(\n","      model.parameters(),\n","      lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(\n","      optimizer,\n","      learning_rate,\n","      epochs=num_epochs,\n","      steps_per_epoch=len(data_loader),\n","      pct_start=0.02,  # Warm up for 2% of the total training time\n","      )\n","  best_metric = 0.0\n","  for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n","    with tqdm.notebook.tqdm(\n","        data_loader,\n","        desc=\"epoch {}\".format(epoch + 1),\n","        unit=\"batch\",\n","        total=len(data_loader)) as batch_iterator:\n","      model.train()\n","      total_loss = 0.0\n","      for i, batch in enumerate(batch_iterator, start=1):\n","        optimizer.zero_grad()\n","        loss = model.compute_loss(batch)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        batch_iterator.set_postfix(mean_loss=total_loss / i)\n","      validation_metric = model.get_validation_metric()\n","      batch_iterator.set_postfix(\n","          mean_loss=total_loss / i,\n","          validation_metric=validation_metric)\n","      if validation_metric > best_metric:\n","        print(\n","            f\"Obtained a new best validation metric of {validation_metric}, saving model \"\n","            f\"checkpoint to {model_file}...\")\n","        torch.save(model.state_dict(), model_file)\n","        best_metric = validation_metric\n","  print(f\"Reloading best model checkpoint from {model_file}...\")\n","  model.load_state_dict(torch.load(model_file))"],"metadata":{"id":"VKc1pwaZzcUN","execution":{"iopub.status.busy":"2022-03-07T18:03:04.527905Z","iopub.execute_input":"2022-03-07T18:03:04.528685Z","iopub.status.idle":"2022-03-07T18:03:04.541552Z","shell.execute_reply.started":"2022-03-07T18:03:04.528643Z","shell.execute_reply":"2022-03-07T18:03:04.540734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define a function that returns the predicted tags themselves."],"metadata":{"id":"_oSUMS_IRFN_"}},{"cell_type":"code","source":["def predict_tags(tagging_model, split, limit=None):\n","  assert split in ('dev', 'test')\n","  sents = READER.sents(split)\n","  dataset = POSTaggingDataset(split)\n","  data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=8, shuffle=False, collate_fn=dataset.collate)\n","  tagging_model.eval()\n","  pred_tagged_sents = []\n","  with torch.no_grad():\n","    for batch in data_loader:\n","      mask = (batch['labels'] != -1)\n","      predicted_labels = tagging_model.encode(batch).argmax(-1)\n","      for i in range(batch['ids'].shape[0]):\n","        example_predicted_tags = [\n","            PARTS_OF_SPEECH[label] for label in predicted_labels[i][mask[i]]]\n","        sent = sents[len(pred_tagged_sents)]\n","        assert len(sent) == len(example_predicted_tags)\n","        pred_tagged_sents.append(list(zip(sent, example_predicted_tags)))\n","        if limit is not None and len(pred_tagged_sents) >= limit:\n","          return pred_tagged_sents\n","  return pred_tagged_sents"],"metadata":{"id":"Gh7Kx_R8VH0Q","execution":{"iopub.status.busy":"2022-03-07T18:03:04.571574Z","iopub.execute_input":"2022-03-07T18:03:04.572039Z","iopub.status.idle":"2022-03-07T18:03:04.580635Z","shell.execute_reply.started":"2022-03-07T18:03:04.572003Z","shell.execute_reply":"2022-03-07T18:03:04.579924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformer POS Tagging Model"],"metadata":{"id":"vb7NfD4BD7I_"}},{"cell_type":"markdown","source":["We now implement a standard Transformer encoder."],"metadata":{"id":"xRLmOHQzE4He"}},{"cell_type":"markdown","source":["The first part is multi-head self-attention. We:\n","- Apply linear projections to convert the feature vector at each token into separate vectors for the query, key, and value.\n","- Apply attention, scaling the logits by $\\frac{1}{sqrt(d_{qkv})}$.\n","- Mask padding tokens so they are never attended to\n","- Perform attention `n_head` times in parallel, where the results are concatenated and then projected using a linear layer\n","\n","We include two types of dropout:\n","- the output of the attention layer (just prior to the residual connection)\n","- the attention probabilites, right after the softmax operation that's applied to query-key dot products\n","\n","We start with the multi-head attention."],"metadata":{"id":"xbL92ahgw09l"}},{"cell_type":"code","source":["import pdb\n","\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self, d_model=256, n_head=4, d_qkv=32, dropout=0.1, **kwargs):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.n_head = n_head\n","    self.d_qkv = d_qkv\n","\n","    self.w_q = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))\n","    self.w_k = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))\n","    self.w_v = nn.Parameter(torch.Tensor(n_head, d_model, d_qkv))\n","    self.w_o = nn.Parameter(torch.Tensor(n_head, d_qkv, d_model))\n","    nn.init.xavier_normal_(self.w_q)\n","    nn.init.xavier_normal_(self.w_k)\n","    nn.init.xavier_normal_(self.w_v)\n","    nn.init.xavier_normal_(self.w_o)\n","\n","    self.attn_drop = nn.Dropout(dropout)\n","    self.output_drop = nn.Dropout(dropout)\n","  \n","  def forward(self, x, mask):\n","    \"\"\"Runs the multi-head self-attention layer.\n","\n","    Args:\n","      x: the input to the layer, a tensor of shape [batch size, length, d_model]\n","      mask: a mask for disallowing attention to padding tokens.\n","    Returns:\n","      A single tensor containing the output from this layer\n","    \"\"\"\n","    # [batch_size, n_head, seq_len, d_qkv]\n","    query = torch.einsum('ijk,mnj -> mink', self.w_q, x)\n","    key = torch.einsum('ijk,mnj -> mink', self.w_k, x)\n","    value = torch.einsum('ijk,mnj -> mink', self.w_v, x)\n","\n","    # [batch_size, n_head, seq_len, seq_len]\n","    score = torch.einsum('ijkl,ijml -> ijkm', query, key)\n","    score /= np.sqrt(self.d_qkv)\n","    # assuming mask has shape [batch_size, seq_len], has 1s for unmasked tokens, and 0s for masked tokens\n","    #masked_score = torch.mul(score, mask.reshape(mask.shape[0], 1, 1, mask.shape[1]))\n","\n","    # assuming mask has shape [batch_size, seq_len], has 1s for masked tokens, and 0s for unmasked tokens\n","    masked_score = score + mask.reshape(mask.shape[0], 1, 1, mask.shape[1])*-1e9\n","    attn = F.softmax(masked_score, -1)\n","    attn = self.attn_drop(attn)\n","  \n","    # [batch_size, n_head, seq_len, d_qkv]\n","    attn_output = torch.einsum('ijkl,ijlm -> ijkm', attn, value)\n","\n","    # w_o: [n_head, d_qkv, d_model]\n","    # sum over heads and d_qkv: [batch_size, seq_len, d_model]\n","    output = torch.einsum('ijkl, jlm -> ikm', attn_output, self.w_o)\n","    output = self.output_drop(output)\n","\n","    return output"],"metadata":{"id":"IZ4v4TKJXzdE","execution":{"iopub.status.busy":"2022-03-07T18:03:04.817732Z","iopub.execute_input":"2022-03-07T18:03:04.818216Z","iopub.status.idle":"2022-03-07T18:03:04.830819Z","shell.execute_reply.started":"2022-03-07T18:03:04.818174Z","shell.execute_reply":"2022-03-07T18:03:04.829856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We now implemennt the position-wise feed forward layer, consisting of two dense linear layers with ReLU nonlinearity in the middle."],"metadata":{"id":"BSLWy_8Ne82f"}},{"cell_type":"code","source":["class PositionwiseFeedForward(nn.Module):\n","  def __init__(self, d_model, d_ff, dropout=0.1):\n","    super().__init__()\n","    self.linear1 = nn.Linear(d_model, d_ff)\n","    self.linear2 = nn.Linear(d_ff, d_model)\n","    self.drop = nn.Dropout(dropout)\n","\n","  def forward(self, x):\n","    output = self.linear2(F.relu(self.linear1(x)))\n","    output = self.drop(output)\n","\n","    return output"],"metadata":{"id":"aYq9acgnXvhk","execution":{"iopub.status.busy":"2022-03-07T18:03:04.832209Z","iopub.execute_input":"2022-03-07T18:03:04.832514Z","iopub.status.idle":"2022-03-07T18:03:04.843843Z","shell.execute_reply.started":"2022-03-07T18:03:04.832469Z","shell.execute_reply":"2022-03-07T18:03:04.843126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Combining the two gives the full transformer encoder architecture."],"metadata":{"id":"Gm0-10wAfA5N"}},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","  def __init__(self, d_model=256, d_ff=1024, n_layers=4, n_head=4, d_qkv=32,\n","               dropout=0.1):\n","    super().__init__()\n","    self.mha_layers = nn.ModuleList([MultiHeadAttention(d_model=d_model, n_head=n_head, d_qkv=d_qkv)]*n_layers)\n","    self.pff_layers = nn.ModuleList([PositionwiseFeedForward(d_model=d_model, d_ff=d_ff)]*n_layers)\n","\n","  def forward(self, x, mask):\n","    \"\"\"Runs the Transformer encoder.\n","\n","    Args:\n","      x: the input to the Transformer, a tensor of shape\n","         [batch size, length, d_model]\n","      mask: a mask for disallowing attention to padding tokens.\n","    Returns:\n","      A single tensor containing the output from the Transformer\n","    \"\"\"\n","\n","    # uses residual connections and layer norms at every multi-head attention\n","    #   and feed-forward layer\n","    for i, mha_layer in enumerate(self.mha_layers):\n","      x = F.layer_norm(self.mha_layers[i](x, mask)+x, x.shape)\n","      x = F.layer_norm(self.pff_layers[i](x)+x, x.shape)\n","\n","    return x"],"metadata":{"id":"CahKqIbqKDTf","execution":{"iopub.status.busy":"2022-03-07T18:03:04.845271Z","iopub.execute_input":"2022-03-07T18:03:04.845799Z","iopub.status.idle":"2022-03-07T18:03:04.856625Z","shell.execute_reply.started":"2022-03-07T18:03:04.845762Z","shell.execute_reply":"2022-03-07T18:03:04.855945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We include positional encodings using a standard position-dependent context."],"metadata":{"id":"m2mLail6UMjN"}},{"cell_type":"code","source":["class AddPositionalEncoding(nn.Module):\n","  def __init__(self, d_model=256, input_dropout=0.1, timing_dropout=0.1,\n","               max_len=512):\n","    super().__init__()\n","    self.timing_table = nn.Parameter(torch.FloatTensor(max_len, d_model))\n","    nn.init.normal_(self.timing_table)\n","    self.input_dropout = nn.Dropout(input_dropout)\n","    self.timing_dropout = nn.Dropout(timing_dropout)\n","  \n","  def forward(self, x):\n","    \"\"\"\n","    Args:\n","      x: A tensor of shape [batch size, length, d_model]\n","    \"\"\"\n","    x = self.input_dropout(x)\n","    timing = self.timing_table[None, :x.shape[1], :]\n","    timing = self.timing_dropout(timing)\n","    return x + timing"],"metadata":{"id":"OQx5ZmlZt0H2","execution":{"iopub.status.busy":"2022-03-07T18:03:04.858229Z","iopub.execute_input":"2022-03-07T18:03:04.858746Z","iopub.status.idle":"2022-03-07T18:03:04.868992Z","shell.execute_reply.started":"2022-03-07T18:03:04.858707Z","shell.execute_reply":"2022-03-07T18:03:04.868287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Combining everything:"],"metadata":{"id":"RK6B7pvEVl-H"}},{"cell_type":"code","source":["class TransformerPOSTaggingModel(POSTaggingModel):\n","  def __init__(self):\n","    super().__init__()\n","    d_model = 256\n","    self.add_timing = AddPositionalEncoding(d_model)\n","    self.encoder = TransformerEncoder(d_model)\n","    self.vocab_size = 16000\n","    self.embed = nn.Embedding(self.vocab_size, d_model)\n","    self.out = nn.Linear(d_model, self.vocab_size)\n","\n","  def encode(self, batch):\n","    \"\"\"\n","    Args:\n","      batch: an input batch as a dictionary; the key 'ids' holds the vocab ids\n","        of the subword tokens in a tensor of size [batch_size, sequence_length]\n","    Returns:\n","      A single tensor containing logits for each subword token.\n","    \"\"\"\n","    mask = (batch['ids']==PAD_ID)\n","\n","    embedded = self.embed(batch['ids'])\n","    positions = self.add_timing(embedded)\n","    encoded = self.encoder(positions, mask)\n","    encoded = F.layer_norm(encoded, encoded.shape)\n","    output = self.out(encoded)\n","\n","    return output"],"metadata":{"id":"inIqeRH6VlfW","execution":{"iopub.status.busy":"2022-03-07T18:03:04.871927Z","iopub.execute_input":"2022-03-07T18:03:04.872163Z","iopub.status.idle":"2022-03-07T18:03:04.881255Z","shell.execute_reply.started":"2022-03-07T18:03:04.872132Z","shell.execute_reply":"2022-03-07T18:03:04.880484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 8\n","batch_size = 16\n","\n","tagging_model = TransformerPOSTaggingModel().to(device)\n","train(tagging_model, num_epochs, batch_size, \"tagging_model.pt\")"],"metadata":{"id":"5o812OQHufeg","execution":{"iopub.status.busy":"2022-03-07T18:40:57.68418Z","iopub.execute_input":"2022-03-07T18:40:57.684783Z","iopub.status.idle":"2022-03-07T18:46:34.090244Z","shell.execute_reply.started":"2022-03-07T18:40:57.684746Z","shell.execute_reply":"2022-03-07T18:46:34.089455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Parsing Setup"],"metadata":{"id":"u15C5gGLMVSa"}},{"cell_type":"markdown","source":["We now implement a parser as a span classification task. Each span in the sentence (that is, each combination of start and end position) will be assigned a label. Constituents will be labeled with their syntactic category, while non-constituents will recieve a special null label.\n","\n","The span \"playing tennis\" is simultaneously a verb phrase (VP) and a nested clause (S). To resolve this issue, we introduce a special chain label \"S+VP\" for this situation.\n","\n","The function `collapse_unary_strip_pos` transforms trees to collapse such unary chains. It also strips part-of-speech labels (which can be predicted by the tagger in the previous part of this project), as well as the root label \"TOP\"."],"metadata":{"id":"6n2YvVNSfkus"}},{"cell_type":"code","source":["def collapse_unary_strip_pos(tree):\n","  def strip_pos(tree):\n","    if len(tree) == 1 and isinstance(tree[0], str):\n","      return tree[0]\n","    else:\n","      return nltk.tree.Tree(tree.label(), [strip_pos(child) for child in tree])\n","  collapsed_tree = strip_pos(tree)\n","  collapsed_tree.collapse_unary(collapsePOS=True)\n","  if collapsed_tree.label() == 'TOP' and len(collapsed_tree) == 1:\n","    collapsed_tree = collapsed_tree[0]\n","  return collapsed_tree "],"metadata":{"id":"XVMG0H5c_SkK","execution":{"iopub.status.busy":"2022-03-07T18:03:06.010294Z","iopub.execute_input":"2022-03-07T18:03:06.011072Z","iopub.status.idle":"2022-03-07T18:03:06.017551Z","shell.execute_reply.started":"2022-03-07T18:03:06.011033Z","shell.execute_reply":"2022-03-07T18:03:06.016745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Child 0 is:', collapsed_tree[0])\n","display(collapsed_tree[0])\n","print('Child 1 is:', collapsed_tree[1])\n","display(collapsed_tree[1])\n","print('Child 1 label is:', collapsed_tree[1].label())"],"metadata":{"id":"tc6GfgWJygrv","executionInfo":{"status":"ok","timestamp":1646787234039,"user_tz":480,"elapsed":296,"user":{"displayName":"Newton Cheng","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07220749752828109188"}},"outputId":"cd975fd0-f75f-4cd0-b29c-6d0da6e424e9","execution":{"iopub.status.busy":"2022-03-07T18:03:06.034218Z","iopub.execute_input":"2022-03-07T18:03:06.035015Z","iopub.status.idle":"2022-03-07T18:03:06.051768Z","shell.execute_reply.started":"2022-03-07T18:03:06.034901Z","shell.execute_reply":"2022-03-07T18:03:06.0511Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":319}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Child 0 is: (NP She)\n"]},{"output_type":"display_data","data":{"text/plain":["Tree('NP', ['She'])"],"image/svg+xml":"<svg baseProfile=\"full\" height=\"72px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,40.0,72.0\" width=\"40px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">She</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Child 1 is: (VP enjoys (S+VP playing (NP tennis)))\n"]},{"output_type":"display_data","data":{"text/plain":["Tree('VP', ['enjoys', Tree('S+VP', ['playing', Tree('NP', ['tennis'])])])"],"image/svg+xml":"<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,200.0,168.0\" width=\"200px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VP</text></svg><svg width=\"32%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">enjoys</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"16%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"68%\" x=\"32%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S+VP</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">playing</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NP</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">tennis</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66%\" y1=\"1.2em\" y2=\"3em\" /></svg>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Child 1 label is: VP\n"]}]},{"cell_type":"markdown","source":["We first implement an `encode_tree` function that maps from Tree objects to sets of spans with labels and starting/end positions. The span start/end position will be defined in terms of subword positions. The start position is inclusive and the end is exclusive."],"metadata":{"id":"AQ9UFCYqLjAZ"}},{"cell_type":"code","source":["from collections import defaultdict\n","  \n","def encode_tree(tree):\n","  \"\"\"Converts a tree into subword token ids and a list of labeled spans.\n","\n","  Args:\n","    tree: an nltk.tree.Tree object\n","\n","  Returns:\n","    A tuple (ids, is_word_end, spans)\n","      ids: a list of token ids in the subword vocabulary\n","      is_word_end: a list of bools indicating if the token is the last in the word\n","      spans: a list of tuples of form (start, end, label), where `start` is\n","             the position in ids where the span starts, `end` is the ending\n","             point in the span (exclusive), and `label` is a string indicating\n","             the syntactic label for the constituent.\n","  \"\"\"\n","  tree = collapse_unary_strip_pos(tree)\n","\n","  # get ids and is_word_end\n","  sent = tree.leaves()\n","  ids = []\n","  is_word_end = []\n","\n","  for word in sent:\n","    word_ids = VOCAB.EncodeAsIds(word)\n","    ids.extend(word_ids)\n","    is_word_end.extend([False] * (len(word_ids) - 1) + [True])\n","\n","  spans = []\n","\n","  # fill out spans with DFS/pre-order traversal\n","  repeats = defaultdict(int)\n","  \n","  def visit_tree(tree):\n","    label = tree.label()\n","    tree_words = tree.leaves()\n","    tree_ids = []\n","\n","    # do this to avoid some edge cases with duplicates\n","    for word in tree_words:\n","      word_ids = VOCAB.EncodeAsIds(word)\n","      tree_ids.extend(word_ids)\n","    \n","    # locate the words within the full sentence\n","    # there's probably a more efficient way to do this\n","    # handle dupes by tracking occurrences in dictionary\n","    span = [(i, i+len(tree_ids)) for i in range(len(ids)) if ids[i:i+len(tree_ids)] == tree_ids]\n","    valid_spans = [(x,y) for x,y in span if is_word_end[y-1]]\n","    if len(valid_spans) > 1:\n","      idx = repeats[tuple(tree_ids)]\n","      spans.append((valid_spans[idx][0], valid_spans[idx][1], label))\n","      repeats[tuple(tree_ids)] += 1\n","    else:\n","      spans.append((valid_spans[0][0], valid_spans[0][1], label))\n","\n","    for subtree in tree:\n","       if type(subtree) == nltk.tree.Tree:\n","         visit_tree(subtree)\n","\n","  visit_tree(tree)\n","\n","  return ids, is_word_end, spans"],"metadata":{"id":"LalYW2FVJyUG","execution":{"iopub.status.busy":"2022-03-07T18:03:06.107091Z","iopub.execute_input":"2022-03-07T18:03:06.107832Z","iopub.status.idle":"2022-03-07T18:03:06.117791Z","shell.execute_reply.started":"2022-03-07T18:03:06.107794Z","shell.execute_reply":"2022-03-07T18:03:06.117017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We obtain a set of all span labels in the training data. We also introduce an `UNK` label. Finally, there is a null label to represent that a span is not a syntactic constituent. We then implement dataset object for the parsing task."],"metadata":{"id":"ssrIa50sKUKi"}},{"cell_type":"code","source":["SPAN_LABELS = set()\n","for tree in READER.parsed_sents('train'):\n","  _, _, spans = encode_tree(tree)\n","  for _, _, label in spans:\n","    SPAN_LABELS.add(label)\n","SPAN_LABELS.add('')\n","SPAN_LABELS.add('UNK')"],"metadata":{"id":"NTWsre3cTJyR","execution":{"iopub.status.busy":"2022-03-07T18:03:07.474602Z","iopub.execute_input":"2022-03-07T18:03:07.47484Z","iopub.status.idle":"2022-03-07T18:03:27.041886Z","shell.execute_reply.started":"2022-03-07T18:03:07.47481Z","shell.execute_reply":"2022-03-07T18:03:27.041163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ParsingDataset(torch.utils.data.Dataset):\n","  def __init__(self, split):\n","    assert split in ('train', 'dev', 'test')\n","    self.trees = READER.parsed_sents(split)\n","    if split == 'train':\n","      # To speed up training, we only train on short sentences.\n","      self.trees = [tree for tree in self.trees if len(tree.leaves()) <= 40]\n","\n","  def __len__(self):\n","    return len(self.trees)\n","\n","  def __getitem__(self, index):\n","    \"\"\" This function loads a single tree into tensors for 'ids', 'labels', and\n","    'is_word_end'.\n","\n","    See 'collate' function below for a description of the batched version of the\n","    tensors to return.\n","    \"\"\"\n","\n","    tree = self.trees[index]\n","    ids, is_word_end, spans = encode_tree(tree)\n","    # adding beginning and ending tokens\n","    ids = [BOS_ID] + ids + [EOS_ID]\n","    ids = torch.LongTensor(ids)\n","\n","    # creating the mask to pair ending subword tokens with full words\n","    is_word_end = [False] + is_word_end + [False]\n","    is_word_end = torch.LongTensor(is_word_end).bool()\n","    labels = torch.zeros(ids.shape[0], ids.shape[0])\n","\n","    # setting diagonal, lower triangle, and first row to -1 for invalid spans\n","    lower_tri = torch.tril(torch.ones_like(labels)).bool()\n","    labels[lower_tri] = -1\n","    labels[0] = torch.full_like(ids, -1)\n","\n","    for x, y, label in spans:\n","      if label in SPAN_SET:\n","        labels[x+1,y+1] = SPAN_LABELS.index(label)\n","      else:\n","        labels[x+1,y+1] = SPAN_LABELS.index('UNK')\n","\n","    return {'ids' : ids, 'labels' : labels, 'is_word_end' : is_word_end}\n","\n","  @staticmethod\n","  def collate(batch):\n","    \"\"\" This function takes a list of examples as output by your __getitem__\n","    function and turns them into batched tensors.\n","    \n","    Returns:\n","      A dictionary with three keys.\n","      * 'ids' is a LongTensor of shape [batch_size, max_sentence_length]\n","      * 'labels' is a LongTensor of shape [batch_size, max_sentence_length, max_sentence_length]\n","        with labels[batch, i, j] representing the label of the span starting at\n","        subword position i and ending at subword position j (exclusive).\n","      * 'is_word_end' is bool tensor of shape [batch_size, max_sentence_length],\n","        with True values at the last sub-word piece for each word.\n","    \"\"\"\n","    ids = pad_sequence([item['ids'] for item in batch], batch_first=True, padding_value=PAD_ID).long()\n","    is_word_end = pad_sequence([item['is_word_end'] for item in batch], batch_first=True, padding_value=False)\n","\n","    max_length = 0\n","    for item in batch:\n","      max_length = max(max_length, item['labels'].shape[0])\n","\n","    labels = []\n","    for item in batch:\n","      pad = max_length-item['labels'].shape[0]\n","      label_tensor = F.pad(item['labels'], (0, pad, 0, pad), value=-1)\n","      labels.append(label_tensor)\n","\n","    labels = torch.stack(labels).long()\n","\n","    return {\n","        'ids': ids.to(device),\n","        'labels': labels.to(device),\n","        'is_word_end': is_word_end.to(device),\n","        }\n"],"metadata":{"id":"jGOw6mS_JH67","execution":{"iopub.status.busy":"2022-03-07T18:03:27.043218Z","iopub.execute_input":"2022-03-07T18:03:27.043437Z","iopub.status.idle":"2022-03-07T18:03:27.06188Z","shell.execute_reply.started":"2022-03-07T18:03:27.043406Z","shell.execute_reply":"2022-03-07T18:03:27.061197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Parsing Model"],"metadata":{"id":"TPWrN4Eb2lN5"}},{"cell_type":"markdown","source":["Now we implement the full parsing model."],"metadata":{"id":"DcmWjcttMRck"}},{"cell_type":"code","source":["class ParsingModel(nn.Module):\n","  def encode(self, batch):\n","    raise NotImplementedError()\n","\n","  def compute_loss(self, batch):\n","    raise NotImplementedError()\n","\n","  def get_validation_metric(self):\n","    dataset = ParsingDataset('dev')\n","    data_loader = torch.utils.data.DataLoader(\n","      dataset, batch_size=8, collate_fn=dataset.collate)\n","    self.eval()\n","    total_gold_spans = 0\n","    total_predicted_spans = 0\n","    total_correct = 0\n","    with torch.no_grad():\n","      for batch in data_loader:\n","        mask = (batch['labels'] != -1)\n","        model_output = self.encode(batch)\n","        predicted_labels = model_output.argmax(-1)\n","        predicted_labels = predicted_labels[mask]\n","        gold_labels = batch['labels'][mask]\n","\n","        total_gold_spans += (gold_labels != 0).sum().item()\n","        total_predicted_spans += (predicted_labels != 0).sum().item()\n","        total_correct += ((predicted_labels == gold_labels) & (gold_labels != 0)\n","            ).sum().item()\n","\n","    if total_predicted_spans != 0:\n","      precision = total_correct / total_predicted_spans\n","    else:\n","      precision = 0.0\n","    recall = total_correct / total_gold_spans\n","    if precision == 0.0 or recall == 0.0:\n","      f1 = 0.0\n","    else:\n","      f1 = 2 * precision * recall / (precision + recall)\n","    # For convenience, we represent precion/recall/F1 as percentage points.\n","    precision *= 100\n","    recall *= 100\n","    f1 *= 100\n","    print(f\"precision={precision:.2f} recall={recall:.2f} f1={f1:.2f}\")\n","    return f1"],"metadata":{"id":"kcT3t6lVz2ij","execution":{"iopub.status.busy":"2022-03-07T18:04:52.482949Z","iopub.execute_input":"2022-03-07T18:04:52.483333Z","iopub.status.idle":"2022-03-07T18:04:52.494198Z","shell.execute_reply.started":"2022-03-07T18:04:52.483291Z","shell.execute_reply":"2022-03-07T18:04:52.493405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The parser operates by:\n","- Running a Transformer encoder to produce a vector at each position in the sentence.\n","- Computing a vector for each span, by subtracting the vectors for the start and endpoints.\n","- Runing an MLP span classifier that takes these span vectors as input."],"metadata":{"id":"XgUDTDfSMh4O"}},{"cell_type":"code","source":["class TransformerParsingModel(ParsingModel):\n","  def __init__(self):\n","    super().__init__()\n","    d_model = 256\n","    self.add_timing = AddPositionalEncoding(d_model)\n","    self.encoder = TransformerEncoder(d_model)\n","    self.vocab_size = 16000\n","    self.label_size = len(SPAN_LABELS)\n","    \n","    self.embed = nn.Embedding(self.vocab_size, d_model)\n","    self.linear1 = nn.Linear(d_model, d_model)\n","    self.linear2 = nn.Linear(d_model, d_model)\n","    self.drop1 = nn.Dropout(0.4)\n","    self.drop2 = nn.Dropout(0.1)\n","    self.out = nn.Linear(d_model, self.label_size)\n","\n","  def encode(self, batch):\n","    \"\"\"Returns logits for each label and each span in the sentence.\n","    \n","    Returns:\n","      A float tensor of shape [batch_size, length, length, len(SPAN_LABELS)],\n","      where the element at position [n, i, j, l] represents the score (logit) of\n","      assigning label l to the span beginning at subword position i and ending\n","      at position j (exclusive), for the n-th example in the batch.\n","    \"\"\"\n","    mask = (batch['ids']==PAD_ID)\n","\n","    embedded = self.embed(batch['ids'])\n","    positions = self.add_timing(embedded)\n","    encoded = self.encoder(positions, mask)\n","    span_vectors = encoded.reshape(encoded.shape[0], 1, encoded.shape[1], -1) - encoded.reshape(encoded.shape[0], encoded.shape[1], 1, -1)\n","\n","    output = self.drop1(F.relu(self.linear1(span_vectors)))\n","    output = self.linear2(output)\n","    output = F.layer_norm(output, output.shape)\n","    output = self.drop2(output)\n","    output = self.out(output)\n","\n","    return output\n","  \n","  def compute_loss(self, batch):\n","    \"\"\"Compute the cross-entropy loss for training the model. -1 labels are\n","      passed over.\n","    \"\"\"\n","    logits = self.encode(batch)\n","    labels = batch['labels']\n","    loss = F.cross_entropy(logits.permute(0, 3, 1, 2), labels, ignore_index=-1, reduction='mean')\n","\n","    return loss"],"metadata":{"id":"X75QefU4N_jj","execution":{"iopub.status.busy":"2022-03-07T18:04:52.495656Z","iopub.execute_input":"2022-03-07T18:04:52.495945Z","iopub.status.idle":"2022-03-07T18:04:52.50972Z","shell.execute_reply.started":"2022-03-07T18:04:52.495875Z","shell.execute_reply":"2022-03-07T18:04:52.508906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 16\n","batch_size = 16\n","\n","parsing_model = TransformerParsingModel().to(device)\n","train(parsing_model, num_epochs, batch_size, \"parsing_model.pt\",\n","      dataset_cls=ParsingDataset)"],"metadata":{"id":"zcZNJ8m-UwRF","execution":{"iopub.status.busy":"2022-03-07T18:04:52.510982Z","iopub.execute_input":"2022-03-07T18:04:52.511312Z","iopub.status.idle":"2022-03-07T18:21:43.437503Z","shell.execute_reply.started":"2022-03-07T18:04:52.511276Z","shell.execute_reply":"2022-03-07T18:21:43.436776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We implement a `predict` function will run your parser on batches from the dataset the uses CKY decoding to produce trees."],"metadata":{"id":"tnSnBH-EMnuQ"}},{"cell_type":"code","source":["def predict(parsing_model, split, tagging_model=None):\n","  assert split in ('dev', 'test')\n","  if tagging_model is None:\n","    tagged_sents = READER.tagged_sents(split)\n","  else:\n","    tagged_sents = predict_tags(tagging_model, split)\n","  \n","  label_scores_charts = predict_span_label_scores(parsing_model, split)\n","\n","  pred_trees = []\n","  for tagged_sent, label_scores_chart in zip(tagged_sents, label_scores_charts):\n","    leaves = [nltk.tree.Tree(tag, [word]) for word, tag in tagged_sent]\n","    tree = cky_decode(leaves, label_scores_chart)\n","    tree = uncollapse_tree(tree)\n","    pred_trees.append(tree)\n","  return pred_trees\n","\n","\n","def predict_span_label_scores(parsing_model, split):\n","  assert split in ('dev', 'test')\n","  dataset = ParsingDataset(split)\n","  data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=8, shuffle=False, collate_fn=dataset.collate)\n","  parsing_model.eval()\n","  all_label_scores_charts = []\n","  with torch.no_grad():\n","    for batch in data_loader:\n","      label_scores_charts = parsing_model.encode(batch)\n","      label_scores_charts = F.log_softmax(label_scores_charts, dim=-1) # Not necessary, but okay to keep. \n","      for i in range(batch['ids'].shape[0]):\n","        label_scores_chart = label_scores_charts[i]\n","\n","        # The data pipeline uses is_word_end for consistency with the part of\n","        # speech tagging models, but here we need is_word_start instead. Note\n","        # that because span endpoints use exclusive indexing, the index actually\n","        # points to the first subword in the next word.\n","        is_word_end = batch['is_word_end'][i]\n","        is_word_start = F.pad(is_word_end, (1, -1), value=False)\n","        is_word_start[1] = True\n","\n","        # Extract scores for whole words only, ignoring any model decisions that\n","        # have a span start or end halfway through a word. Evaluation for\n","        # parsing typically uses the ground-truth tokenization from the dataset.\n","        label_scores_chart = label_scores_chart[\n","            is_word_start, : ,:][:, is_word_start, :]\n","        label_scores_chart = label_scores_chart.cpu().numpy()\n","\n","        all_label_scores_charts.append(label_scores_chart)\n","  return all_label_scores_charts\n","\n","\n","def uncollapse_tree(tree):\n","  if isinstance(tree, str):\n","    return tree\n","  else:\n","    labels = tree.label().split('+')\n","    children = []\n","    for child in tree:\n","      child = uncollapse_tree(child)\n","      if isinstance(child, str) and (len(tree) > 1\n","                                     or labels[-1] not in PARTS_OF_SPEECH):\n","        child = nltk.tree.Tree('UNK', [child])\n","      children.append(child)\n","    for label in labels[::-1]:\n","      children = [nltk.tree.Tree(label, children)]\n","    return children[0]"],"metadata":{"id":"RGk0toh0OIDK","execution":{"iopub.status.busy":"2022-03-07T18:21:43.438984Z","iopub.execute_input":"2022-03-07T18:21:43.439682Z","iopub.status.idle":"2022-03-07T18:21:43.454764Z","shell.execute_reply.started":"2022-03-07T18:21:43.43964Z","shell.execute_reply":"2022-03-07T18:21:43.453962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The argmax model predictions are not guaranteed to be a valid tree: some of the spans may intersect with one another, which is not allowed in our syntactic formalism. We implement CKY decoding to find the highest-scoring tree under the model.\n","\n","CKY is designed to handle binary trees, but some productions have greater than two children. For this project, we handle non-binary productions by allowing intermediate dummy nodes with a special null label, implemented in the code as an empty string '' at index 0 in SPAN_LABELS. We allow the algorithm to select spans with the null label, but then collapse them out when creating the tree.  \n","\n","To avoid having high-probability null labels competing with non-null labels, we normalize our scores by subtracting the logit of the null label from the scores of all the labels. This normalization makes it so that if a local span decision would prefer a non-null label it will have positive score, and thus have higher score than all 0-score null labels."],"metadata":{"id":"xDMZUA84NM2R"}},{"cell_type":"code","source":["def cky_decode(leaves, label_scores_chart):\n","  label_scores_chart = torch.from_numpy(label_scores_chart)\n","  label_scores_chart = label_scores_chart - label_scores_chart[:,:,0].unsqueeze(-1)\n","  n = label_scores_chart.shape[0]\n","  pointers = torch.ones(n,n).long()*-1\n","  scores = torch.ones(n, n)*-1e9\n","  labels = torch.zeros(n,n).long()\n","\n","  for i in range(n-1):\n","    scores[i, i+1], labels[i,i+1] = torch.max(label_scores_chart[i,i+1], 0)\n","    pointers[i,i+1] = i\n","\n","  for diff in range(2,n):\n","    for i in range(0,n-diff):\n","      j = i + diff\n","      base_score, labels[i,j] = torch.max(label_scores_chart[i,j], 0)\n","      pointers[i,j] = i+1\n","      for k in range(i+1, j):\n","        split_score = scores[i,k] + scores[k,j]\n","        if scores[i,j] < base_score + split_score:\n","          scores[i,j] = base_score + split_score\n","          pointers[i,j] = k\n","\n","\n","  def constructTree(i,j):\n","    k = pointers[i,j]\n","    label_idx = labels[i,j]\n","\n","    if j-i > 1:\n","      left_tree = constructTree(i,k)\n","      right_tree = constructTree(k,j)\n","      tree = nltk.tree.Tree(SPAN_LABELS[label_idx], [])\n","      \n","      if left_tree.label() == '':\n","        for child in left_tree:\n","          tree.append(child)\n","      else:\n","        tree.append(left_tree)\n","        \n","      if right_tree.label() == '':\n","        for child in right_tree:\n","          tree.append(child)\n","      else:\n","        tree.append(right_tree)\n","\n","    else:\n","      subtree = [leaves[k]]\n","      tree = nltk.tree.Tree(SPAN_LABELS[label_idx], subtree)\n","\n","    return tree\n","\n","  tree = constructTree(0, n-1)\n","\n","  return tree"],"metadata":{"id":"Bk1v6zptPrtL","execution":{"iopub.status.busy":"2022-03-07T18:21:43.456344Z","iopub.execute_input":"2022-03-07T18:21:43.456961Z","iopub.status.idle":"2022-03-07T18:21:43.470811Z","shell.execute_reply.started":"2022-03-07T18:21:43.456898Z","shell.execute_reply":"2022-03-07T18:21:43.47004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicted_dev_trees = predict(parsing_model, 'dev')\n","predicted_dev_trees[10]"],"metadata":{"id":"T3CLSr4cQODq","execution":{"iopub.status.busy":"2022-03-07T18:21:43.47213Z","iopub.execute_input":"2022-03-07T18:21:43.472598Z","iopub.status.idle":"2022-03-07T18:21:55.01442Z","shell.execute_reply.started":"2022-03-07T18:21:43.472562Z","shell.execute_reply":"2022-03-07T18:21:55.013701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Final evaluation"],"metadata":{"id":"TFbnEobgQIWx"}},{"cell_type":"markdown","source":["The standard evaluation for parsing is typically performed using the EVALB software (https://nlp.cs.nyu.edu/evalb/). \n","\n","The metrics reported by EVALB include:\n","- *Bracketing Recall*: Number of correct constituents divided by the number of constituents in the ground-truth data\n","- *Bracketing Precision*: Number of correct constituents divided by the number of constituents in the predicted trees\n","- *Bracketing FMeasure*: The F1 score, which is the harmonic mean of the Bracketing Recall and Bracketing Precision\n","- *Complete Match*: Percentage of sentences where recall and precision are both 100%\n","- *Average crossing*: Number of constituents crossing a ground-truth constituent divided by the number of sentences\n","- *No crossing*: Percentage of sentences which have 0 crossing brackets\n","\n","Metrics are reported both for the full dataset and for the subset of sentences that have length 40 or shorter."],"metadata":{"id":"gb5gwdPjQ9gi"}},{"cell_type":"code","source":["# compile the EVALB program\n","!cd EVALB; make "],"metadata":{"id":"gNePLbPGQkrk","execution":{"iopub.status.busy":"2022-03-07T18:21:55.015782Z","iopub.execute_input":"2022-03-07T18:21:55.016202Z","iopub.status.idle":"2022-03-07T18:21:56.153333Z","shell.execute_reply.started":"2022-03-07T18:21:55.016161Z","shell.execute_reply":"2022-03-07T18:21:56.152488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('./dev_predictions_parser_only.txt', 'w') as f:\n","  for tree in predicted_dev_trees:\n","    f.write(' '.join(str(tree).split()) + '\\n')"],"metadata":{"id":"m7vBflpYQKof","execution":{"iopub.status.busy":"2022-03-07T18:21:56.155339Z","iopub.execute_input":"2022-03-07T18:21:56.155707Z","iopub.status.idle":"2022-03-07T18:21:56.906581Z","shell.execute_reply.started":"2022-03-07T18:21:56.155664Z","shell.execute_reply":"2022-03-07T18:21:56.905788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!EVALB/evalb -p EVALB/nk.prm dev dev_predictions_parser_only.txt | tail -n 29"],"metadata":{"id":"G-DeuFtciydn","execution":{"iopub.status.busy":"2022-03-07T18:21:56.907704Z","iopub.execute_input":"2022-03-07T18:21:56.908175Z","iopub.status.idle":"2022-03-07T18:21:57.960664Z","shell.execute_reply.started":"2022-03-07T18:21:56.908134Z","shell.execute_reply":"2022-03-07T18:21:57.95983Z"},"trusted":true},"execution_count":null,"outputs":[]}]}