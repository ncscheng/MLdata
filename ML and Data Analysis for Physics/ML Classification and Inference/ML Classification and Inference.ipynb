{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Project3_288.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pBqZZgW2PYGa"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OxfPHCFPYGT"
      },
      "source": [
        "## Project 3\n",
        "\n",
        "## <em> Classification and inference with machine learning</em>\n",
        "<br>\n",
        "This project is a standard application of classification and inference methods to the MNIST data set.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c45_KzLbPYGU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpURe5cYPYGU"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJe3gVaGPYGU"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.integrate import quad\n",
        "#For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "YSsTBBSUPYGU"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLOKYxT5PYGU"
      },
      "source": [
        "#### Problem 1 - Using Keras - MNIST\n",
        "\n",
        "The goal of this notebook is to introduce deep neural networks (DNNs) and convolutional neural networks (CNNs) using the high-level Keras package and to become familiar with how to choose its architecture, cost function, and optimizer in Keras. We will also learn how to train neural networks.\n",
        "\n",
        "We will once again work with the MNIST dataset of hand written digits introduced in earlier HW. The goal is to find a statistical model which recognizes and distinguishes between the ten handwritten digits (0-9).\n",
        "\n",
        "The MNIST dataset comprises handwritten digits, each of which comes in a square image, divided into a $28\\times 28$ pixel grid. Every pixel can take on $256$ nuances of the gray color, interpolating between white and black, and hence each data point assumes any value in the set $\\{0,1,\\dots,255\\}$. Since there are $10$ categories in the problem, corresponding to the ten digits, this problem represents a generic classification task. \n",
        "\n",
        "In this Notebook, we show how to use the Keras python package to tackle the MNIST problem with the help of deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsQkAr0KPYGU"
      },
      "source": [
        "## Creating DNNs with Keras\n",
        "\n",
        "Constructing a Deep Neural Network to solve ML problems is a multiple-stage process. Quite generally, one can identify the key steps as follows:\n",
        "\n",
        "* ***step 1:*** Load and process the data\n",
        "* ***step 2:*** Define the model and its architecture\n",
        "* ***step 3:*** Choose the optimizer and the cost function\n",
        "* ***step 4:*** Train the model \n",
        "* ***step 5:*** Evaluate the model performance on the *unseen* test data\n",
        "* ***step 6:*** Modify the hyperparameters to optimize performance for the specific data set\n",
        "\n",
        "We would like to emphasize that, while it is always possible to view steps 1-5 as independent of the particular task we are trying to solve, it is only when they are put together in ***step 6*** that the real gain of using Deep Learning is revealed, compared to less sophisticated methods such as the regression models. With this remark in mind, we shall focus predominantly on steps 1-5 below. We show how one can use grid search methods to find optimal hyperparameters in ***step 6***."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je2THnXiPYGV"
      },
      "source": [
        "### Step 1: Load and Process the Data\n",
        "\n",
        "Keras knows to download automatically the MNIST data from the web. All we need to do is import the `mnist` module and use the `load_data()` class, and it will create the training and test data sets or us.\n",
        "\n",
        "The MNIST set has pre-defined test and training sets, in order to facilitate the comparison of the performance of different models on the data.\n",
        "\n",
        "Once we have loaded the data, we need to format it in the correct shape ($({\\mathrm{N_{samples}}}, {\\mathrm{N_{features}}})$). \n",
        "\n",
        "The size of each sample, i.e. the number of bare features used is N_features (whis is 784 because we have a $28 \\times 28$ pixel grid), while the number of potential classification categories is \"num_classes\" (which is 10, number of digits).\n",
        "\n",
        "Each pixel\n",
        "contains a greyscale value quantified by an integer between\n",
        "0 and 255. To standardize the dataset, we normalize\n",
        "the input data in the interval [0, 1]. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LdyZ6cpPYGV"
      },
      "source": [
        "!pip install tensorflow==1.15.0\n",
        "!pip install keras==2.3.0\n",
        "!pip install scikit-learn==0.19"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLumviT3e_t6"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras,sklearn\n",
        "# suppress tensorflow compilation warnings\n",
        "import os\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "seed=0\n",
        "np.random.seed(seed) # fix random seed\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfXiOou1PYGV"
      },
      "source": [
        "# input image dimensions\n",
        "num_classes = 10 # 10 digits\n",
        "\n",
        "img_rows, img_cols = 28, 28 # number of pixels \n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "X_train = X_train[:40000]\n",
        "Y_train = Y_train[:40000]\n",
        "\n",
        "# reshape data, depending on Keras backend\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows*img_cols)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)\n",
        "    \n",
        "# cast floats to single precesion\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# rescale data in interval [0,1]\n",
        "X_train /= 255\n",
        "X_test /= 255\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y59w5p7mPYGV"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 1. Make a plot of one MNIST digit (2D plot using X data - make sure to reshape it into a $28 \\times 28$ matrix) and label it (which digit does it correspond to?). </i></span> <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36yktBi4PYGV"
      },
      "source": [
        "X_0 = X_train[0,:]\n",
        "X_0 = X_0.reshape((img_rows, img_cols))\n",
        "Y_0 = Y_train[0]\n",
        "\n",
        "plt.imshow(X_0, cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "print(f'The label is {Y_0}.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfbXUwn9PYGV"
      },
      "source": [
        "Last, we cast the label vectors $y$ to binary class matrices (a.k.a. one-hot format)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKdTHQ1SPYGV"
      },
      "source": [
        "# convert class vectors to binary class matrices\n",
        "\n",
        "print(\"before conversion - \")\n",
        "print(\"y vector : \", Y_train[0:10])\n",
        "\n",
        "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
        "Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
        "\n",
        "print(\"after conversion - \")\n",
        "print(\"y vector : \", Y_train[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AGmfXONPYGV"
      },
      "source": [
        "Here in this template, we use 40000 training samples and 10000 test samples. Remember that we preprocessed data into the shape $({\\mathrm{N_{samples}}}, {\\mathrm{N_{features}}})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hllfgLXjPYGW"
      },
      "source": [
        "print('X_train shape:', X_train.shape)\n",
        "print('Y_train shape:', Y_train.shape)\n",
        "print()\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbKb1z23PYGW"
      },
      "source": [
        "### Step 2: Define the Neural Net and its Architecture\n",
        "\n",
        "We can now move on to construct our deep neural net. We shall use Keras's `Sequential()` class to instantiate a model, and will add different deep layers one by one.\n",
        "\n",
        "Let us create an instance of Keras' `Sequential()` class, called `model`. As the name suggests, this class allows us to build DNNs layer by layer. (https://keras.io/getting-started/sequential-model-guide/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfWUS09JPYGW"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "# instantiate model\n",
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1RNWO9CPYGW"
      },
      "source": [
        "We use the `add()` method to attach layers to our model. For the purposes of our introductory example, it suffices to focus on `Dense` layers for simplicity. (https://keras.io/layers/core/) Every `Dense()` layer accepts as its first required argument an integer which specifies the number of neurons. The type of activation function for the layer is defined using the `activation` optional argument, the input of which is the name of the activation function in `string` format. Examples include `relu`, `tanh`, `elu`, `sigmoid`, `softmax`.\n",
        "\n",
        "In order for our DNN to work properly, we have to make sure that the numbers of input and output neurons for each layer match. Therefore, we specify the shape of the input in the first layer of the model explicitly using the optional argument `input_shape=(N_features,)`. The sequential construction of the model then allows Keras to infer the correct input/output dimensions of all hidden layers automatically. Hence, we only need to specify the size of the softmax output layer to match the number of categories.\n",
        "\n",
        "First, add a `Dense` layer with 400 output neurons and `relu` activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0hLjPavPYGW"
      },
      "source": [
        "model.add(Dense(400,input_shape=(img_rows*img_cols,), activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6YX5GKDPYGW"
      },
      "source": [
        "Add another layer with 100 output neurons. Then, we will apply \"dropout,\" a regularization scheme that has been widely adopted in the neural networks literature: during the training procedure neurons\n",
        "are randomly “dropped out” of the neural network with some\n",
        "probability $p$ giving rise to a thinned network. It prevents overfitting by reducing spurious correlations between neurons within the network by introducing\n",
        "a randomization procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s2T7K-HPYGW"
      },
      "source": [
        "model.add(Dense(100, activation='relu'))\n",
        "# apply dropout with rate 0.5\n",
        "model.add(Dropout(0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gnKtSHfPYGW"
      },
      "source": [
        "Lastly, we need to add a soft-max layer since we have a multi-class output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOGQZ9ETPYGW"
      },
      "source": [
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qIjfbhdPYGW"
      },
      "source": [
        "### Step 3: Choose the Optimizer and the Cost Function\n",
        "\n",
        "Next, we choose the loss function according to which to train the DNN. For classification problems, this is the cross entropy, and since the output data was cast in categorical form, we choose the `categorical_crossentropy` defined in Keras' `losses` module. Depending on the problem of interest one can pick any other suitable loss function. To optimize the weights of the net, we choose SGD. This algorithm is already available to use under Keras' `optimizers` module (https://keras.io/optimizers/), but we could use `Adam()` or any other built-in one as well. The parameters for the optimizer, such as `lr` (learning rate) or `momentum` are passed using the corresponding optional arguments of the `SGD()` function. \n",
        "\n",
        "While the loss function and the optimizer are essential for the training procedure, to test the performance of the model one may want to look at a particular `metric` of performance. For instance, in categorical tasks one typically looks at their `accuracy`, which is defined as the percentage of correctly classified data points. \n",
        "\n",
        "To complete the definition of our model, we use the `compile()` method, with optional arguments for the `optimizer`, `loss`, and the validation `metric` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgFOEXGtPYGX"
      },
      "source": [
        "# compile the model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='SGD', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7Js_DKcPYGX"
      },
      "source": [
        "### Step 4: Train the model\n",
        "\n",
        "We train our DNN in minibatches. Shuffling the training data during training improves stability of the model. Thus, we train over a number of training epochs. \n",
        "\n",
        "(The number of epochs is the number of complete passes through the training dataset, and the batch size is a number of samples propagated through the network before the model is updated.)\n",
        "\n",
        "Training the DNN is a one-liner using the `fit()` method of the `Sequential` class. The first two required arguments are the training input and output data. As optional arguments, we specify the mini-`batch_size`, the number of training `epochs`, and the test or validation data. To monitor the training procedure for every epoch, we set `verbose=True`. \n",
        "\n",
        "Let us set `batch_size` = 64 and `epochs` = 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxkBZ0CgPYGX"
      },
      "source": [
        "# training parameters\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# train DNN and store training info in history\n",
        "history=model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
        "          verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CWxh01cPYGX"
      },
      "source": [
        "### Step 5: Evaluate the Model Performance on the *Unseen* Test Data\n",
        "\n",
        "Next, we evaluate the model and read of the loss on the test data, and its accuracy using the `evaluate()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW6wICs5PYGX"
      },
      "source": [
        "# evaluate model\n",
        "score = model.evaluate(X_test, Y_test, verbose=1)\n",
        "\n",
        "# print performance\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# look into training history\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.ylabel('model accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='best')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('model loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aK6mHN7PYGX"
      },
      "source": [
        "### Step 6: Modify the Hyperparameters to Optimize Performance of the Model\n",
        "\n",
        "Last, we show how to use the grid search option of scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to optimize the \n",
        "hyperparameters of our model.\n",
        "\n",
        "First, define a function for crating a DNN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1QhfW42PYGX"
      },
      "source": [
        "def create_DNN(optimizer=keras.optimizers.Adam()):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(400,input_shape=(img_rows*img_cols,), activation='relu'))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWQegOH2PYGX"
      },
      "source": [
        "With epochs = 1 and batch_size = 64, do grid search over the following optimization schemes: ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dKOz3PSqZ0e"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "H1T6l5z5PYGX"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 1\n",
        "model_gridsearch = KerasClassifier(build_fn=create_DNN, \n",
        "                        epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# list of allowed optional arguments for the optimizer, see `compile_model()`\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "# define parameter dictionary\n",
        "param_grid = dict(optimizer=optimizer)\n",
        "\n",
        "# call scikit grid search module\n",
        "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
        "grid_result = grid.fit(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wNpnzkAPYGX"
      },
      "source": [
        "Show the mean test score of all optimization schemes and determine which scheme gives the best accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxQgu3ysPYGX"
      },
      "source": [
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECL3J1J5PYGX"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 2. Create a DNN with one Dense layer having 200 output neurons. Do the grid search over any 5 different activation functions from https://keras.io/activations/. Let epochs = 1, batches = 64, p_dropout=0.5, and optimizer=keras.optimizers.Adam().  Make sure to print the mean test score of each case and determine which activation functions gives the best accuracy. </i></span> <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdRw2SvtPYGY"
      },
      "source": [
        "def create_DNN2(activation):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(200,input_shape=(img_rows*img_cols,), activation=activation))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x6Yor8EwM8V"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 1\n",
        "\n",
        "model_gridsearch = KerasClassifier(build_fn=create_DNN2, \n",
        "                        epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "activation = ['relu','tanh','sigmoid','softplus','selu']\n",
        "param_grid = dict(activation=activation)\n",
        "\n",
        "grida = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
        "grid_resulta = grida.fit(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLFTlH_LyBms"
      },
      "source": [
        "print(\"Best: %f using %s\" % (grid_resulta.best_score_, grid_resulta.best_params_))\n",
        "means = grid_resulta.cv_results_['mean_test_score']\n",
        "stds = grid_resulta.cv_results_['std_test_score']\n",
        "params = grid_resulta.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaVNAp3DPYGY"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 3. Now, do the grid search over different combination of batch sizes (10, 30, 50, 100) and number of epochs (1, 2, 5). Use the activation function that gave you the highest accuracy in Part 2. Make sure to print the mean test score of each case and determine which combination of batch size and number of epochs gives the best accuracy. Here, you have a freedom to create your own DNN (assume an arbitrary number of Dense layers, optimization scheme, etc).   </i></span> <br>\n",
        "\n",
        "Hint: To do the grid search over both batch_size and epochs, you can do:\n",
        "\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w87WI5kePYGY"
      },
      "source": [
        "batch_size = [10,30,50,100]\n",
        "epochs = [1,2,5]\n",
        "\n",
        "model_gridsearch = KerasClassifier(build_fn=create_DNN, \n",
        "                        epochs=1, batch_size=64, verbose=1)\n",
        "\n",
        "param_grid = dict(batch_size=batch_size,epochs=epochs)\n",
        "\n",
        "gridb = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
        "gridb_result = gridb.fit(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLLgujjgz4eh"
      },
      "source": [
        "print(\"Best: %f using %s\" % (gridb_result.best_score_, gridb_result.best_params_))\n",
        "means = gridb_result.cv_results_['mean_test_score']\n",
        "stds = gridb_result.cv_results_['std_test_score']\n",
        "params = gridb_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psKBtdpnPYGY"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 4. Do the grid search over the number of neurons in the Dense layer and make a plot of mean test score as a function of num_neurons. Again, you have a freedom to create your own DNN. </i></span> <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvNDAu6eEYfm"
      },
      "source": [
        "def create_DNN3(num_neurons):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_neurons,input_shape=(img_rows*img_cols,), activation='relu'))\n",
        "    model.add(Dense(int(np.rint(num_neurons/4)), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbpBrAtqPYGY"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 1\n",
        "num_neurons = range(100,1000,100)\n",
        "\n",
        "model_gridsearch = KerasClassifier(build_fn=create_DNN3, \n",
        "                        epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "param_grid = dict(num_neurons=num_neurons)\n",
        "\n",
        "gridc = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
        "gridc_result = gridc.fit(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv3WWCMJTo6i"
      },
      "source": [
        "print(\"Best: %f using %s\" % (gridc_result.best_score_, gridc_result.best_params_))\n",
        "means = gridc_result.cv_results_['mean_test_score']\n",
        "stds = gridc_result.cv_results_['std_test_score']\n",
        "params = gridc_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlA107kfPYGY"
      },
      "source": [
        "## Creating CNNs with Keras\n",
        "\n",
        "We have so far considered each MNIST data sample as a $(28\\times 28,)$-long 1d vector. This approach neglects any spatial structure in the image. On the other hand, we do know that in every one of the hand-written digits there are *local* spatial correlations between the pixels, which we would like to take advantage of to improve the accuracy of our classification model. To this end, we first need to reshape the training and test input data as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8lSEULqPYGY"
      },
      "source": [
        "# reshape data, depending on Keras backend\n",
        "if keras.backend.image_data_format() == 'channels_first':\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "    \n",
        "print('X_train shape:', X_train.shape)\n",
        "print('Y_train shape:', Y_train.shape)\n",
        "print()\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEqL7jbMPYGY"
      },
      "source": [
        "One can ask the question of whether a neural net can learn to recognize such local patterns. This can be achieved by using convolutional layers. Luckily, all we need to do is change the architecture of our DNN.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1ZQFqg-GCRKufS4mMGcluqBquv7yCPKlX)\n",
        "\n",
        "After we instantiate the model, add the first convolutional layer with 10 filters, which is the dimensionality of output space. (https://keras.io/layers/convolutional/) Here, we will be concerned with local spatial filters\n",
        "that take as inputs a small spatial patch of the\n",
        "previous layer at all depths. We consider a three-dimensional kernel of size $5\\times5\\times1$. Check out this visualization of the\n",
        "convolution procedure for a square input of unit depth: https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
        "The convolution consists of running this filter over all locations\n",
        "in the spatial plane. After computing the filter, the output is passed through\n",
        "a non-linearity, a ReLU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVvVRxqZPYGZ"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(10, kernel_size=(5, 5),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6GR8mlJPYGZ"
      },
      "source": [
        "Subsequently, add a 2D pooling layer. (https://keras.io/layers/pooling/) This pooling layer coarse-grain spatial information by performing\n",
        "a subsampling at each depth. Here, we use the the max pool operation. In a max pool, the spatial\n",
        "dimensions are coarse-grained by replacing a small region\n",
        "(say $2\\times2$ neurons) by a single neuron whose output is the\n",
        "maximum value of the output in the region."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "962uordOPYGZ"
      },
      "source": [
        "model.add(MaxPooling2D(pool_size=(2, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TftnYfaPYGZ"
      },
      "source": [
        "Add another convolutional layers with 20 filters and apply dropout. Then, add another pooling layer and flatten the data. You can do DNNs afterwards and compile the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuTxBWB2PYGZ"
      },
      "source": [
        "# add second convolutional layer with 20 filters\n",
        "model.add(Conv2D(20, (5, 5), activation='relu'))\n",
        "# apply dropout with rate 0.5\n",
        "model.add(Dropout(0.5))\n",
        "# add 2D pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# flatten data\n",
        "model.add(Flatten())\n",
        "# add a dense all-to-all relu layer\n",
        "model.add(Dense(20*4*4, activation='relu'))\n",
        "# apply dropout with rate 0.5\n",
        "model.add(Dropout(0.5))\n",
        "# soft-max layer\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQoXP5B1PYGZ"
      },
      "source": [
        "Lastly, train your CNN and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieBRP0e3PYGZ"
      },
      "source": [
        "# training parameters\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "# train CNN\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(X_test, Y_test))\n",
        "\n",
        "# evaluate model\n",
        "score = model.evaluate(X_test, Y_test, verbose=1)\n",
        "\n",
        "# print performance\n",
        "print()\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB34cePSPYGZ"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 5. Do the grid search over any 3 different optimization schemes and 2 activation functions. Suppose that we have a 2 convolutional layers with 10 neurons. Let p_dropout = 0.5, epochs = 1, and batch_size = 64. Determine which combination of optimization scheme and activation function gives the best accuracy. </i></span> <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVlMmaYcPYGZ"
      },
      "source": [
        "def conv_model(optimizer,activation):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(10, (5, 5),activation=activation,input_shape=input_shape))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Conv2D(10, (5, 5), activation=activation))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(200, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6bK2i-NiuFC"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 1\n",
        "\n",
        "optimizer = ['Adam','Adamax','SGD']\n",
        "activation = ['relu','tanh']\n",
        "\n",
        "model_gridsearch = KerasClassifier(build_fn=conv_model, \n",
        "                        epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "param_grid = dict(optimizer=optimizer,activation=activation)\n",
        "\n",
        "grid = GridSearchCV(estimator=model_gridsearch, param_grid=param_grid, n_jobs=1, cv=4)\n",
        "grid_result = grid.fit(X_train,Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz8L1mKK1gAt"
      },
      "source": [
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhUgnockPYGZ"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 6. Create an arbitrary DNN (you are free to choose any activation function, optimization scheme, etc) and evaluate its performance. Then, add two convolutional layers and pooling layers and evaluate its performance again. How do they compare? </i></span> <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y03_Z1nOomfw"
      },
      "source": [
        "# basic DNN model using relu for activation, Adam for optimizing, and accuracy for the performance metric\n",
        "model = Sequential()\n",
        "model.add(Dense(200,input_shape=(img_rows*img_cols,), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer='Adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIzQaSPZqkaR"
      },
      "source": [
        "# reshaping data for DNN\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows*img_cols)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows*img_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvID0fP9PYGZ"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 4\n",
        "\n",
        "model.fit(X_train, Y_train,batch_size=batch_size,epochs=epochs,verbose=1)\n",
        "\n",
        "# evaluating model\n",
        "score = model.evaluate(X_test, Y_test, verbose=1)\n",
        "\n",
        "print('No convolution - Test loss:', score[0])\n",
        "print('No convolution - Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zci1zKMk0cy3"
      },
      "source": [
        "# reshaping for CNN\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-vck2Zizi9E"
      },
      "source": [
        "# adding convolutional layers to the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(10, (5, 5),activation='relu',input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# 2nd conv layer with dropout\n",
        "model.add(Conv2D(20, (5, 5), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# same DNN as before\n",
        "model.add(Flatten())\n",
        "model.add(Dense(200, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,optimizer='Adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArznF1rzoBTE"
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 4\n",
        "\n",
        "model.fit(X_train, Y_train,batch_size=batch_size,epochs=epochs,verbose=1)\n",
        "\n",
        "# evaluating model\n",
        "score = model.evaluate(X_test, Y_test, verbose=1)\n",
        "\n",
        "print('With convolution - Test loss:', score[0])\n",
        "print('With convolution - Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di3NXD3_8eej"
      },
      "source": [
        "Adding the convolution and pooling layers has improved the accuracy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBqZZgW2PYGa"
      },
      "source": [
        "#### Problem 2 - Using Tensorflow - Ising Model\n",
        "\n",
        "Next, we show how one can use deep neural nets to classify the states of the 2D Ising model according to their phase. This should be compared with the use of logistic-regression in earlier HW.\n",
        "\n",
        "The Hamiltonian for the classical Ising model is given by\n",
        "\n",
        "$$ H = -J\\sum_{\\langle ij\\rangle}S_{i}S_j,\\qquad \\qquad S_j\\in\\{\\pm 1\\} $$\n",
        "\n",
        "where the lattice site indices $i,j$ run over all nearest neighbors of a 2D square lattice, and $J$ is some arbitrary interaction energy scale. We adopt periodic boundary conditions. Onsager proved that this model undergoes a phase transition in the thermodynamic limit from an ordered ferromagnet with all spins aligned to a disordered phase at the critical temperature $T_c/J=2/\\log(1+\\sqrt{2})\\approx 2.26$. For any finite system size, this critical point is expanded to a critical region around $T_c$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRnLyclIPYGa"
      },
      "source": [
        "### Step 1: Load and Process the Data\n",
        "\n",
        "We begin by writing a `DataSet` class and two functions `read_data_sets` and `load_data` to process the 2D Ising data. \n",
        "\n",
        "The `DataSet` class performs checks on the data shape and casts the data into the correct data type for the calculation. It contains a function method called `next_batch` which shuffles the data and returns a mini-batch of a pre-defined size. This structure is particularly useful for the training procedure in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6Hgcbg_PYGa"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "seed=12\n",
        "np.random.seed(seed)\n",
        "import sys, os, argparse\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import dtypes\n",
        "# suppress tflow compilation warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "tf.set_random_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOSCYK9uPYGa"
      },
      "source": [
        "class DataSet(object):\n",
        "\n",
        "    def __init__(self,data_X,data_Y,dtype=dtypes.float32):\n",
        "        \"\"\"Checks data and casts it into correct data type. \"\"\"\n",
        "\n",
        "        dtype = dtypes.as_dtype(dtype).base_dtype\n",
        "        if dtype not in (dtypes.uint8, dtypes.float32):\n",
        "            raise TypeError('Invalid dtype %r, expected uint8 or float32' % dtype)\n",
        "\n",
        "        assert data_X.shape[0] == data_Y.shape[0], ('data_X.shape: %s data_Y.shape: %s' % (data_X.shape, data_Y.shape))\n",
        "        self.num_examples = data_X.shape[0]\n",
        "\n",
        "        if dtype == dtypes.float32:\n",
        "            data_X = data_X.astype(np.float32)\n",
        "        self.data_X = data_X\n",
        "        self.data_Y = data_Y \n",
        "\n",
        "        self.epochs_completed = 0\n",
        "        self.index_in_epoch = 0\n",
        "\n",
        "    def next_batch(self, batch_size, seed=None):\n",
        "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
        "\n",
        "        if seed:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        start = self.index_in_epoch\n",
        "        self.index_in_epoch += batch_size\n",
        "        if self.index_in_epoch > self.num_examples:\n",
        "            # Finished epoch\n",
        "            self.epochs_completed += 1\n",
        "            # Shuffle the data\n",
        "            perm = np.arange(self.num_examples)\n",
        "            np.random.shuffle(perm)\n",
        "            self.data_X = self.data_X[perm]\n",
        "            self.data_Y = self.data_Y[perm]\n",
        "            # Start next epoch\n",
        "            start = 0\n",
        "            self.index_in_epoch = batch_size\n",
        "            assert batch_size <= self.num_examples\n",
        "        end = self.index_in_epoch\n",
        "\n",
        "        return self.data_X[start:end], self.data_Y[start:end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhQ6CJHzPYGa"
      },
      "source": [
        "Now, load the Ising dataset, and splits it into three subsets: ordered, critical and disordered, depending on the temperature which sets the distribution they are drawn from. Once again, we use the ordered and disordered data to create a training and a test data set for the problem. Classifying the states in the critical region is expected to be harder and we only use this data to test the performance of our model in the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EIhxVoFPYGa"
      },
      "source": [
        "\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "import collections\n",
        "\n",
        "L=40 # linear system size\n",
        "\n",
        "# load data\n",
        "fac = 25\n",
        "file_name = \"/content/drive/My Drive/P188_288/P188_288_Project3/Ising2DFM_reSample_L40_T=All.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
        "data = pickle.load(open(file_name,'rb')) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
        "data = data[::fac]\n",
        "data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
        "data=data.astype('int')\n",
        "data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
        "\n",
        "file_name = \"/content/drive/My Drive/P188_288/P188_288_Project3/Ising2DFM_reSample_L40_T=All_labels.pkl\" # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
        "labels = pickle.load(open(file_name,'rb')) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
        "\n",
        "# divide data into ordered, critical and disordered\n",
        "X_ordered=data[:int(70000/fac),:]\n",
        "Y_ordered=labels[:70000][::fac]\n",
        "\n",
        "X_critical=data[int(70000/fac):int(100000/fac),:]\n",
        "Y_critical=labels[70000:100000][::fac]\n",
        "\n",
        "X_disordered=data[int(100000/fac):,:]\n",
        "Y_disordered=labels[100000:][::fac]\n",
        "\n",
        "del data,labels\n",
        "\n",
        "# define training and test data sets\n",
        "X=np.concatenate((X_ordered,X_disordered)) #np.concatenate((X_ordered,X_critical,X_disordered))\n",
        "Y=np.concatenate((Y_ordered,Y_disordered)) #np.concatenate((Y_ordered,Y_critical,Y_disordered))\n",
        "\n",
        "del X_ordered, X_disordered, Y_ordered, Y_disordered\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "732xLZy1PYGa"
      },
      "source": [
        "# pick random data points from ordered and disordered states to create the training and test sets\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size=0.6)\n",
        "\n",
        "\n",
        "# make data categorical\n",
        "Y_train=to_categorical(Y_train)\n",
        "Y_test=to_categorical(Y_test)\n",
        "Y_critical=to_categorical(Y_critical)\n",
        "\n",
        "\n",
        "# create data sets\n",
        "train = DataSet(X_train, Y_train, dtype=dtypes.float32)\n",
        "test = DataSet(X_test, Y_test, dtype=dtypes.float32)\n",
        "critical = DataSet(X_critical, Y_critical, dtype=dtypes.float32)\n",
        "\n",
        "Datasets = collections.namedtuple('Datasets', ['train', 'test', 'critical'])\n",
        "Dataset = Datasets(train=train, test=test, critical=critical)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCsXXavcPYGa"
      },
      "source": [
        "You can load the training data in the following way: (Dataset.train.data_X, Dataset.train.data_Y)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTpGzIzWPYGa"
      },
      "source": [
        "### Steps 2+3: Define the Neural Net and its Architecture, Choose the Optimizer and the Cost Function\n",
        "\n",
        "We can now move on to construct our deep neural net using TensorFlow. \n",
        "\n",
        "Unique for TensorFlow is creating placeholders for the variables of the model, such as the feed-in data `X` and `Y` or the dropout probability `dropout_keepprob` (which has to be set to unity explicitly during testing). Another peculiarity is using the `with` scope to give names to the most important operators. While we do not discuss this here, TensorFlow also allows one to visualise the computational graph for the model (see package documentation on [https://www.tensorflow.org/](https://www.tensorflow.org/)).\n",
        "\n",
        "The shape of X is only partially defined. We know that it will be a matrix, with instances along the first dimension and features along the second dimension, and we know that the number of features is going to be $28\\times28$, but we don't know yet how many instances each training batch will contain. So the shape of X is (None, n_inputs). Similarly, we know that Y will be a vector with one entry per instance, but again we don't know the size of the training batch, so the shape is (None)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eia4k9afPYGa"
      },
      "source": [
        "L=40 # system linear size\n",
        "n_feats=L**2 # 40x40 square lattice\n",
        "n_categories=2 # 2 Ising phases: ordered and disordered\n",
        "\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 2\n",
        "\n",
        "with tf.name_scope('data'):\n",
        "    X=tf.placeholder(tf.float32, shape=(None,n_feats))\n",
        "    Y=tf.placeholder(tf.float32, shape=(None,n_categories))\n",
        "    dropout_keepprob=tf.placeholder(tf.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1yqw4gLPYGa"
      },
      "source": [
        "To classify whether a given spin configuration is in the ordered or disordered phase, we construct a minimalistic model for a DNN with a single hidden layer containing $N_\\mathrm{neurons}$ (which is kept variable so we can try out the performance of different sizes for the hidden layer). \n",
        "\n",
        "Let us use a neuron_layer() function to create layers in the neural nets.\n",
        "\n",
        "1. First, create a name scope using the name of the layer.\n",
        "2. Get the number of inputs by looking up the input matrix's shape and getting the size of the second dimension.\n",
        "3. Create a $W$ variable which holds the weight matrix (i.e. kernel).  Initialize it randomly, using a truncated normal distribution. \n",
        "4. Create a $b$ variable for biases, initialized to 0.\n",
        "5. Create a subgraph to compute $Z=XW+b$\n",
        "6. Use activation function if provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJtO_tCLPYGa"
      },
      "source": [
        "def neuron_layer(X, n_neuron, name, activation = None):\n",
        "    with tf.name_scope(name):\n",
        "        n_inputs = int(X.get_shape()[1])\n",
        "        stddev = 2 / np.sqrt(n_inputs + n_neuron)\n",
        "        init = tf.truncated_normal((n_inputs, n_neuron), stddev = stddev)\n",
        "        W = tf.Variable(init, name = \"kernel\")\n",
        "        b = tf.Variable(tf.zeros([n_neuron]), name = \"bias\")\n",
        "        Z = tf.matmul(X, W) + b\n",
        "        if activation is not None:\n",
        "            return activation(Z)\n",
        "        else:\n",
        "            return Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myMi068pPYGa"
      },
      "source": [
        "Using a neuron_layer() function, create two hidden layers and an output layer. The first hidden layer takes X as its input, and the second takes the output of the first hidden layer as its input. Finally, the output layer takes the output of the second hidden layer as its input.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NGiH9C3PYGa"
      },
      "source": [
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.nn.relu)\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.nn.relu)\n",
        "    logits = tf.layers.dense(hidden2, n_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KVEE13HPYGa"
      },
      "source": [
        "Then, define the cost function that we will use to train the neural net model. Here, use the cross entropy to penalize models that estimate a low probability for the target class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQvoE92nPYGa"
      },
      "source": [
        "with tf.name_scope('loss'):\n",
        "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits)\n",
        "    loss = tf.reduce_mean(xentropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSYIC72LPYGa"
      },
      "source": [
        "Then, define a GradientDescentOptimizer that will tweak the model parameters to minimize the cost function. Now, set learning_rate = 1e-6. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7EXf1nPPYGa"
      },
      "source": [
        "learning_rate = 1e-6\n",
        "with tf.name_scope('optimiser'):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Sd8MxXPYGa"
      },
      "source": [
        "Lastly, specify how to evaluate the model. Let us simply use accuracy as our performance measure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmKdJaeDPYGa"
      },
      "source": [
        "\n",
        "with tf.name_scope('accuracy'):\n",
        "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "    correct_prediction = tf.cast(correct_prediction, tf.float64) # change data type\n",
        "#     correct_prediction = tf.nn.in_top_k(logits, Y, 1)\n",
        "    accuracy = tf.reduce_mean(correct_prediction)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eduZHh7WPYGa"
      },
      "source": [
        "### Steps 4+5: Train the Model and Evaluate its Performance\n",
        "\n",
        "We train our DNN using mini-batches of size $100$ over a total of $100$ epochs, which we define first. We then set up the optimizer parameter dictionary `opt_params`, and use it to create a DNN model. \n",
        "\n",
        "Running TensorFlow requires opening up a `Session` which we abbreviate as `sess` for short. All operations are performed in this session by calling the `run` method. First, we initialize the global variables in TensorFlow's computational graph by running the `global_variables_initializer`. To train the DNN, we loop over the number of epochs. In each fix epoch, we use the `next_batch` function of the `DataSet` class we defined above to create a mini-batch. The forward and backward passes through the weights are performed by running the `loss` and `optimizer` methods. To pass the mini-batch as well as any other external parameters, we use the `feed_dict` dictionary. Similarly, we evaluate the model performance, by getting `accuracy` on the same minibatch data. Note that the dropout probability for testing is set to unity. \n",
        "\n",
        "Once we have exhausted all training epochs, we test the final performance on the entire training, test and critical data sets. This is done in the same way as above.\n",
        "\n",
        "Last, we return the loss and accuracy for each of the training, test and critical data sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh-UVC8dPYGa"
      },
      "source": [
        "training_epochs=100\n",
        "batch_size=100\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # initialize the necessary variables, in this case, w and b\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # train the DNN\n",
        "    for epoch in range(training_epochs): \n",
        "\n",
        "        batch_X, batch_Y = Dataset.train.next_batch(batch_size)\n",
        "\n",
        "        sess.run(optimizer, feed_dict={X: batch_X,Y: batch_Y,dropout_keepprob: 0.5})\n",
        "        \n",
        "\n",
        "    # test DNN performance on entire train test and critical data sets\n",
        "    train_loss, train_accuracy = sess.run([loss, accuracy], \n",
        "                                                feed_dict={X: Dataset.train.data_X, \n",
        "                                                           Y: Dataset.train.data_Y,\n",
        "                                                           dropout_keepprob: 0.5}\n",
        "                                                            )\n",
        "    print(\"train loss/accuracy:\", train_loss, train_accuracy)\n",
        "\n",
        "    test_loss, test_accuracy = sess.run([loss, accuracy], \n",
        "                                                feed_dict={X: Dataset.test.data_X,\n",
        "                                                           Y: Dataset.test.data_Y,\n",
        "                                                           dropout_keepprob: 1.0}\n",
        "                                                           )\n",
        "\n",
        "    print(\"test loss/accuracy:\", test_loss, test_accuracy)\n",
        "\n",
        "    critical_loss, critical_accuracy = sess.run([loss, accuracy], \n",
        "                                                feed_dict={X: Dataset.critical.data_X,\n",
        "                                                           Y: Dataset.critical.data_Y,\n",
        "                                                           dropout_keepprob: 1.0}\n",
        "                                                           )\n",
        "    print(\"crtitical loss/accuracy:\", critical_loss, critical_accuracy)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oomP356FPYGa"
      },
      "source": [
        "### Step 6: Modify the Hyperparameters to Optimize Performance of the Model\n",
        "\n",
        "To study the dependence of our DNN on some of the hyperparameters, we do a grid search over the number of neurons (initially set as 100) in the hidden layer, and different SGD learning rates (initially set as 1e-6). These searches are best done over logarithmically-spaced points. \n",
        "\n",
        "To do this, define a function for creating a DNN model: `create_DNN` and for evaluating the performance: `evaluate_model`.\n",
        "\n",
        "The function `grid_search` will output 2D heat map to show how accuracy changes with learning rate and number of neurons. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzC8Uc8ZPYGa"
      },
      "source": [
        "def create_DNN(n_hidden1=100, n_hidden2=100, learning_rate=1e-6, activation='relu'):\n",
        "    with tf.name_scope('data'):\n",
        "        X=tf.placeholder(tf.float32, shape=(None,n_feats))\n",
        "        Y=tf.placeholder(tf.float32, shape=(None,n_categories))\n",
        "        dropout_keepprob=tf.placeholder(tf.float32)\n",
        "\n",
        "    with tf.name_scope(\"dnn\"):\n",
        "        hidden1 = tf.layers.dense(X, n_hidden1, activation = tf.keras.activations.deserialize(activation))\n",
        "        hidden2 = tf.layers.dense(hidden1, n_hidden2, activation = tf.keras.activations.deserialize(activation))\n",
        "        logits = tf.layers.dense(hidden2, n_outputs)\n",
        "        \n",
        "    with tf.name_scope('loss'):\n",
        "        xentropy = tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits)\n",
        "        loss = tf.reduce_mean(xentropy)\n",
        "        \n",
        "    with tf.name_scope('optimiser'):\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss) \n",
        "\n",
        "    with tf.name_scope('accuracy'):\n",
        "        correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "        correct_prediction = tf.cast(correct_prediction, tf.float64) # change data type\n",
        "    #     correct_prediction = tf.nn.in_top_k(logits, Y, 1)\n",
        "        accuracy = tf.reduce_mean(correct_prediction)\n",
        "        \n",
        "    return X, Y, dropout_keepprob, loss, optimizer, accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnI4X4wkPYGa"
      },
      "source": [
        "def evaluate_model(neurons=100,lr=1e-6,activation=activation,training_epochs=100,batch_size=100):\n",
        "\n",
        "    X, Y, dropout_keepprob, loss, optimizer, accuracy = create_DNN(n_hidden1=neurons, n_hidden2=neurons, learning_rate=lr,activation=activation)\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "        # initialize the necessary variables, in this case, w and b\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # train the DNN\n",
        "        for epoch in range(training_epochs): \n",
        "\n",
        "            batch_X, batch_Y = Dataset.train.next_batch(batch_size)\n",
        "\n",
        "            sess.run(optimizer, feed_dict={X: batch_X,Y: batch_Y,dropout_keepprob: 0.5})\n",
        "\n",
        "\n",
        "        # test DNN performance on entire train test and critical data sets\n",
        "        train_loss, train_accuracy = sess.run([loss, accuracy], \n",
        "                                                    feed_dict={X: Dataset.train.data_X, \n",
        "                                                               Y: Dataset.train.data_Y,\n",
        "                                                               dropout_keepprob: 0.5}\n",
        "                                                                )\n",
        "        print(\"train loss/accuracy:\", train_loss, train_accuracy)\n",
        "\n",
        "        test_loss, test_accuracy = sess.run([loss, accuracy], \n",
        "                                                    feed_dict={X: Dataset.test.data_X,\n",
        "                                                               Y: Dataset.test.data_Y,\n",
        "                                                               dropout_keepprob: 1.0}\n",
        "                                                               )\n",
        "\n",
        "        print(\"test loss/accuracy:\", test_loss, test_accuracy)\n",
        "\n",
        "        critical_loss, critical_accuracy = sess.run([loss, accuracy], \n",
        "                                                    feed_dict={X: Dataset.critical.data_X,\n",
        "                                                               Y: Dataset.critical.data_Y,\n",
        "                                                               dropout_keepprob: 1.0}\n",
        "                                                               )\n",
        "        print(\"crtitical loss/accuracy:\", critical_loss, critical_accuracy)\n",
        "\n",
        "    return train_loss,train_accuracy,test_loss,test_accuracy,critical_loss,critical_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAenw-PyPYGb"
      },
      "source": [
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data(x,y,data, title):\n",
        "\n",
        "    # plot results\n",
        "    fontsize=16\n",
        "\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(data, interpolation='nearest', vmin=0, vmax=1)\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # put text on matrix elements\n",
        "    for i, x_val in enumerate(np.arange(len(x))):\n",
        "        for j, y_val in enumerate(np.arange(len(y))):\n",
        "            c = \"${0:.1f}\\\\%$\".format( 100*data[j,i])  \n",
        "            ax.text(x_val, y_val, c, va='center', ha='center')\n",
        "\n",
        "    # convert axis vaues to to string labels\n",
        "    x=[str(i) for i in x]\n",
        "    y=[str(i) for i in y]\n",
        "\n",
        "\n",
        "    ax.set_xticklabels(['']+x)\n",
        "    ax.set_yticklabels(['']+y)\n",
        "\n",
        "    ax.set_xlabel('$\\\\mathrm{learning\\\\ rate}$',fontsize=fontsize)\n",
        "    ax.set_ylabel('$\\\\mathrm{hidden\\\\ neurons}$',fontsize=fontsize)\n",
        "    \n",
        "    ax.set_title(title,fontsize=fontsize)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJsG3qBdPYGa"
      },
      "source": [
        "def grid_search():\n",
        "    \"\"\"This function performs a grid search over a set of different learning rates \n",
        "    and a number of hidden layer neurons.\"\"\"\n",
        "\n",
        "    # perform grid search over learnign rate and number of hidden neurons\n",
        "    N_neurons=[100, 200, 300, 400, 500]\n",
        "    learning_rates=np.logspace(-6,-1,6)\n",
        "\n",
        "    # pre-alocate variables to store accuracy and loss data\n",
        "    train_loss=np.zeros((len(N_neurons),len(learning_rates)),dtype=np.float64)\n",
        "    train_accuracy=np.zeros_like(train_loss)\n",
        "    test_loss=np.zeros_like(train_loss)\n",
        "    test_accuracy=np.zeros_like(train_loss)\n",
        "    critical_loss=np.zeros_like(train_loss)\n",
        "    critical_accuracy=np.zeros_like(train_loss)\n",
        "\n",
        "    # do grid search\n",
        "    for i, neurons in enumerate(N_neurons):\n",
        "        for j, lr in enumerate(learning_rates):\n",
        "\n",
        "            print(\"training DNN with %4d neurons and SGD lr=%0.6f.\" %(neurons,lr) )\n",
        "\n",
        "            train_loss[i,j],train_accuracy[i,j],\\\n",
        "            test_loss[i,j],test_accuracy[i,j],\\\n",
        "            critical_loss[i,j],critical_accuracy[i,j] = evaluate_model(neurons,lr)\n",
        "\n",
        "    %matplotlib inline\n",
        "    plot_data(learning_rates,N_neurons,train_accuracy, \"training data\")\n",
        "    plot_data(learning_rates,N_neurons,test_accuracy, \"test data\")\n",
        "    plot_data(learning_rates,N_neurons,critical_accuracy, \"critical data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "myhV1jjfPYGb"
      },
      "source": [
        "grid_search()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvE4qwfhPYGb"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 1. Do the grid search over 5 different types of activation functions (https://www.tensorflow.org/api_guides/python/nn#Activation_Functions). Evaluate the performance for each case and determine which gives the best accuracy. You can assume an arbitrary DNN. Show results for training, test, and critical data. </i></span> <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KtWzfLxPYGb"
      },
      "source": [
        "def grid_search_act():\n",
        "    \"\"\"This function performs a grid search over 5 different activation functions.\"\"\"\n",
        "\n",
        "    # perform grid search over activation functions\n",
        "    activations=['relu', 'tanh', 'sigmoid', 'selu', 'softplus']\n",
        "\n",
        "    # pre-allocate variables to store accuracy and loss data\n",
        "    train_loss=np.zeros(len(activations),dtype=np.float64)\n",
        "    train_accuracy=np.zeros_like(train_loss)\n",
        "    test_loss=np.zeros_like(train_loss)\n",
        "    test_accuracy=np.zeros_like(train_loss)\n",
        "    critical_loss=np.zeros_like(train_loss)\n",
        "    critical_accuracy=np.zeros_like(train_loss)\n",
        "\n",
        "    # do grid search\n",
        "    for i, activation in enumerate(activations):\n",
        "        print(f\"Training DNN with activation function {activation}.\")\n",
        "\n",
        "        train_loss[i],train_accuracy[i],\\\n",
        "        test_loss[i],test_accuracy[i],\\\n",
        "        critical_loss[i],critical_accuracy[i] = evaluate_model(neurons=100,lr=0.1,activation=activation)\n",
        "\n",
        "    best_crit = activations[np.argmax(critical_accuracy)]\n",
        "\n",
        "    print(f'The best performance on the critical data is {best_crit}.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOmCx_v-CcI_"
      },
      "source": [
        "grid_search_act()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3hkeMfQPYGb"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 2. Do the grid search over 5 different numbers of epochs and batch sizes. Make a 2D heat map as shown in the example. You can assume an arbitrary DNN. Show results for training, test, and critical data.  </i></span> <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l728UR6Q6oSt"
      },
      "source": [
        "def grid_search_size():\n",
        "    \"\"\"This function performs a grid search over 5 different epoch numbers and 5 different batch sizes\"\"\"\n",
        "\n",
        "    # perform grid search over epochs and batch sizes\n",
        "    epochs = [1,10,20,50,100]\n",
        "    batch_sizes = [10,20,50,80,100]\n",
        "\n",
        "    # pre-alocate variables to store accuracy and loss data\n",
        "    train_loss=np.zeros((len(epochs),len(batch_sizes)),dtype=np.float64)\n",
        "    train_accuracy=np.zeros_like(train_loss)\n",
        "    test_loss=np.zeros_like(train_loss)\n",
        "    test_accuracy=np.zeros_like(train_loss)\n",
        "    critical_loss=np.zeros_like(train_loss)\n",
        "    critical_accuracy=np.zeros_like(train_loss)\n",
        "\n",
        "    # do grid search\n",
        "    for i, epoch in enumerate(epochs):\n",
        "        for j, size in enumerate(batch_sizes):\n",
        "\n",
        "            print(f\"Training DNN with {epoch} epochs and batch size {size}.\")\n",
        "\n",
        "            train_loss[i,j],train_accuracy[i,j],\\\n",
        "            test_loss[i,j],test_accuracy[i,j],\\\n",
        "            critical_loss[i,j],critical_accuracy[i,j] = evaluate_model(neurons=100,lr=0.1,activation='softplus',training_epochs=epoch,batch_size=size)\n",
        "\n",
        "    return train_accuracy, test_accuracy, critical_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSpDGHUeE8nh"
      },
      "source": [
        "train_accuracy, test_accuracy, critical_accuracy = grid_search_size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5x9WYcQPMV2"
      },
      "source": [
        "def plot_sizedata(x,y,data, title):\n",
        "\n",
        "    # plot results\n",
        "    fontsize=16\n",
        "\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(data, interpolation='nearest', vmin=0, vmax=1)\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # put text on matrix elements\n",
        "    for i, x_val in enumerate(np.arange(len(x))):\n",
        "        for j, y_val in enumerate(np.arange(len(y))):\n",
        "            c = \"${0:.1f}\\\\%$\".format( 100*data[j,i])  \n",
        "            ax.text(x_val, y_val, c, va='center', ha='center')\n",
        "\n",
        "    # convert axis vaues to to string labels\n",
        "    x=[str(i) for i in x]\n",
        "    y=[str(i) for i in y]\n",
        "\n",
        "\n",
        "    ax.set_xticklabels(['']+x)\n",
        "    ax.set_yticklabels(['']+y)\n",
        "\n",
        "    ax.set_xlabel('Epochs',fontsize=fontsize)\n",
        "    ax.set_ylabel('Batch size',fontsize=fontsize)\n",
        "    \n",
        "    ax.set_title(title,fontsize=fontsize)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgh4TtyxPSC3"
      },
      "source": [
        "epochs = [1,10,20,50,100]\n",
        "batch_sizes = [10,20,50,80,100]\n",
        "\n",
        "plot_sizedata(batch_sizes,epochs,train_accuracy,'Training Data')\n",
        "plot_sizedata(batch_sizes,epochs,train_accuracy,'Training Data')\n",
        "plot_sizedata(batch_sizes,epochs,train_accuracy,'Training Data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9uo9hhCPYGb"
      },
      "source": [
        "#### Problem 3 - SDSS galaxies\n",
        "\n",
        "The data is provided in the file <b>\"specz_data.txt\"</b>. The columns of the file (length of 13) correspond to - <br>\n",
        "spectroscopic redshift ('zspec'), RA, DEC, magnitudes in 5 bands - u, g, r, i, z (denoted as 'mu,' 'mg,' 'mr,' 'mi,' 'mz' respectively); Exponential and de Vaucouleurs model magnitude fits ('logExp' and 'logDev' http://www.sdss.org/dr12/algorithms/magnitudes/); zebra fit ('pz_zebra); Neural Network fit ('pz_NN') and its error estimate ('pz_NN_Err') <br>\n",
        "\n",
        "We will undertake 2 exercises  - \n",
        "- Regression\n",
        "    - We will use the magnitude of object in different bands ('mu, mg, mr, mi, mz') and do a regression exercise to estimate the redshift of the object. Hence our feature space is 5.\n",
        "    - The correct redshift is given by 'zspec', which is the spectroscopic redshift of the object. We will use this for training and testing purpose. \n",
        "    \n",
        "    Sidenote: Photometry vs. Spectroscopy\n",
        "    \n",
        "    <i>&nbsp; &nbsp; The amount of energy we receive from celestial objects – in the form of radiation – is called the flux, and an astro- nomical technique of measuring the flux is photometry. Flux is usually measured over broad wavelength bands, and with the estimate of the distance to an object, it can infer the object’s luminosity, temperature, size, etc. Usually light is passed through colored filters, and we measure the intensity of the filtered light. \n",
        "    \n",
        "    &nbsp; &nbsp; On the other hand, spectroscopy deals with the spectrum of the emitted light. This tells us what the object is made of, how it is moving, the pressure of the material in it, etc. Note that for faint objects making photometric observation is much easier.\n",
        "    \n",
        "    &nbsp; &nbsp; Photometric redshift (photoz) is an estimate of the distance to the object using photometry. Spectroscopic redshift observes the object’s spectral lines and measures their shifts due to the Doppler effect to infer the distance.</i>\n",
        "    \n",
        "\n",
        "- Classification\n",
        "    - We will use the same magnitudes and now also the redshift of the object  ('zspec') to classify the object as either Elleptical or Spiral. Hence our feature space is now 6.\n",
        "    - The correct class is given by compring 'logExp' and 'logDev' which are the fits for Exponential and Devocular profiles. If logExp > logDev, its a spiral and vice-versa. We will use this for training and testing purpose. Since the classes are not explicitly given, generate a column for those (Classes can be $\\pm 1$. If it is $0$, it does not belong to either of the class.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCwx7ljIPYGb"
      },
      "source": [
        "##### Cleaning\n",
        "\n",
        "Read in the files to create the data (X and Y) for both regression and classification. <br>\n",
        "You will have to clean the data - \n",
        "- Drop the entries that are nan or infinite\n",
        "- Drop the unrealistic numbers such as 999, -999; and magnitudes that are unrealistic. Since these are absolute magnitudes, they should be positive and high. Lets choose a magnitude limit of 15 as safe bet.\n",
        "- For classification, drop the entries that do not belong to either of the class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzrDHtvBPYGb"
      },
      "source": [
        "#Read in and create data\n",
        "\n",
        "fname = '/content/drive/My Drive/P188_288/P188_288_Project3/specz_data.txt'\n",
        "spec_dat=np.genfromtxt(fname,names=True)\n",
        "print(spec_dat.dtype.fields.keys())\n",
        "#convenience variable\n",
        "zspec = spec_dat['zspec']\n",
        "pzNN = spec_dat['pz_NN']\n",
        "#some N redshifts are not defined\n",
        "pzNN[pzNN < 0] = np.nan\n",
        "\n",
        "#For Regression\n",
        "bands = ['u', 'g', 'r','i', 'z' ]\n",
        "mlim = 15\n",
        "\n",
        "xdata = np.concatenate([[spec_dat['m%s'%i] for i in bands]]).T\n",
        "bad = (xdata[:, 0] < mlim) | (xdata[:, 1] < mlim) | (xdata[:, 2] < mlim) & (xdata[:, 3] < mlim) | (xdata[:, 4] < mlim)\n",
        "xdata = xdata[~bad]\n",
        "xdata[xdata<0] = 0\n",
        "ydata = zspec[~bad]\n",
        "\n",
        "#For classification\n",
        "classes = np.sign(spec_dat['logExp'] - spec_dat['logDev'])\n",
        "tmp = np.concatenate([[spec_dat['m%s'%i] for i in bands]]).T\n",
        "xxdata = np.concatenate([tmp, zspec.reshape(-1, 1)], axis=1)\n",
        "bad = (classes==0) | (xxdata[:, 0] < mlim) | (xxdata[:, 1] < mlim) | (xxdata[:, 2] < mlim) & (xxdata[:, 3] < mlim) | (xxdata[:, 4] < mlim)\n",
        "xxdata = xxdata[~bad]\n",
        "classes = classes[~bad]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAd8mWgFPYGb"
      },
      "source": [
        "For regression, X and Y data (called \"xdata\" and \"ydata,\" respectively) is cleaned magnitudes (5 feature space) and spectroscopic redshifts respectively.\n",
        "For classification, X and Y data (called \"xxdata\" and \"classes\" respectively) is cleaned magnitudes+spectroscopic redshifts respectively (6 feature space) and classees respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7smMZLfuPYGb"
      },
      "source": [
        "print('For Regression:')\n",
        "print('Before: Size of datasets is ', zspec.shape[0])\n",
        "print('After: Size of datasets is ', xdata.shape[0])\n",
        "print('')\n",
        "print('For Classification:')\n",
        "print('Before: Size of datasets is ', zspec.shape[0])\n",
        "print('After: Size of datasets is ', xxdata.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDJNlxrWPYGb"
      },
      "source": [
        "##### Visualization\n",
        "\n",
        "The next step should be to visualize the data. <br>\n",
        "For regression\n",
        "- Make a histogram for the distribution of the data (spectroscopic redshift). \n",
        "- Make 5 2D histograms of the distribution of the magnitude as function of redshift (Hint: https://matplotlib.org/devdocs/api/_as_gen/matplotlib.axes.Axes.hist2d.html)\n",
        "\n",
        "For classification <br>\n",
        "- Make 6 1-d histogram for the distribution of the data (6 features - zspec and 5 magnitudes) for both class 1 and -1 separately \n",
        "\n",
        "<span style=\"color:blue\"> <i> 1. Make histograms for both regression and classification. </i></span> <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_7z_EdlPYGb"
      },
      "source": [
        "# distribution of spectroscopic redshift\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.hist(ydata)\n",
        "\n",
        "plt.title('Spectroscopic redshift distribution',fontsize=16)\n",
        "plt.xlabel('Redshift',fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPzcej5Pcbgo"
      },
      "source": [
        "# 2D histogram of redshift vs magnitude\n",
        "for i in range(5):\n",
        "    plt.hist2d(xdata[:,i], ydata)\n",
        "    plt.title(f'Redshift vs. {bands[i]} band magnitude distribution',fontsize=14)\n",
        "    plt.xlabel('Magnitude',fontsize=12)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwkv1zsofxzo"
      },
      "source": [
        "# distribution of magnitudes and redshift according to classification\n",
        "for i in range(6):\n",
        "    plt.hist(xxdata[:,i][classes>0])\n",
        "\n",
        "    if i < 5:\n",
        "        plt.title(f'Class +1 {bands[i]} band magnitude distribution',fontsize=14)\n",
        "        plt.xlabel('Magnitude',fontsize=12)\n",
        "\n",
        "    if i == 5:\n",
        "        plt.title(f'Class +1 Redshift distribution',fontsize=14)\n",
        "        plt.xlabel('Redshift',fontsize=12)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCU17ekxY-L1"
      },
      "source": [
        "for i in range(6):\n",
        "    plt.hist(xxdata[:,i][classes<0])\n",
        "\n",
        "    if i < 5:\n",
        "        plt.title(f'Class -1 {bands[i]} band magnitude distribution',fontsize=14)\n",
        "        plt.xlabel('Magnitude',fontsize=12)\n",
        "\n",
        "    if i == 5:\n",
        "        plt.title(f'Class -1 redshift distribution',fontsize=14)\n",
        "        plt.xlabel('Redshift',fontsize=12)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msYDo7qCPYGb"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 2. Do the following preprocessing: </i></span> <br>\n",
        "\n",
        "#####  Preprocessing:\n",
        "\n",
        "- Next, split the sample into training data and the testing data. We will be using the training data to train different algorithms and then compare the performance over the testing data. In this project, keep 80% data as training data and uses the remaining 20% data for testing.  <br>\n",
        "- Often, the data can be ordered in a specific manner, hence shuffle the data prior to splitting it into training and testing samples. <br>\n",
        "- Many algorithms are also not scale invariant, and hence scale the data (different features to a uniform scale). All this comes under preprocessing the data.\n",
        "http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing <br>\n",
        "Use StandardScaler from sklearn (or write your own routine) to center the data to 0 mean and 1 variance. Note that you only center the training data and then use its mean and variance to scale the testing data before using it. <br><br>\n",
        "\n",
        "Hint: How to get a scaled training data: <br>\n",
        "\n",
        "1. Let the training data be: train = (\"training X data\", \"training Y data\")<br>\n",
        "2. You can first define a StandardScaler: <br>\n",
        "scale_xdata, scale_ydata = preprocessing.StandardScaler(), preprocessing.StandardScaler()<br>\n",
        "3. Then, do the fit: <br>\n",
        "for regression: scale_xdata.fit(train_regression[0]), scale_ydata.fit(train_regression[1].reshape(-1, 1))<br>\n",
        "for classication: scale_xdata.fit(train_classification[0])<br>\n",
        "  Here, no need to fit for y data for classification (it's either +1 or -1. Already scaled)<br>\n",
        "4. Next, transform: <br>\n",
        " for regression: scaled_train_data = (scale_xdata.fit_transform(train_regression[0]), scale_ydata.fit_transform(train_regression[1].reshape(-1, 1)))<br>\n",
        " for classication: scaled_train_data = (scale_xdata.fit_transform(train_classification[0]), train_classification[1])<br>\n",
        " Again, y data is already scaled for classification.  <br>\n",
        "\n",
        "\n",
        "Do this for test data as well.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkNHzAM3PYGb"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOmG0E0i3F_h"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(xdata, ydata, train_size = 0.8,random_state=1)\n",
        "XX_train, XX_test, C_train, C_test = train_test_split(xxdata, classes, train_size = 0.8,random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dblctH1yPYGb"
      },
      "source": [
        "train_reg = (X_train,Y_train)\n",
        "train_cla = (XX_train,C_train)\n",
        "test_reg = (X_test,Y_test)\n",
        "test_cla = (XX_test,C_test)\n",
        "\n",
        "scale_xdata,scale_ydata,scale_xxdata = preprocessing.StandardScaler(),preprocessing.StandardScaler(),preprocessing.StandardScaler()\n",
        "\n",
        "scale_xdata.fit(train_reg[0]), scale_ydata.fit(train_reg[1].reshape(-1,1))\n",
        "scale_xxdata.fit(train_cla[0])\n",
        "\n",
        "sc_train_reg = (scale_xdata.fit_transform(train_reg[0]),scale_ydata.fit_transform(train_reg[1].reshape(-1,1)))\n",
        "sc_train_cla = (scale_xxdata.fit_transform(train_cla[0]), train_cla[1])\n",
        "\n",
        "\n",
        "sc_test_reg = (scale_xdata.transform(test_reg[0]), scale_ydata.transform(test_reg[1].reshape(-1,1)))\n",
        "sc_test_cla = (scale_xxdata.transform(test_cla[0]),test_cla[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5VSVFQzPYGc"
      },
      "source": [
        "##### Metrics\n",
        "\n",
        "The last remaining preperatory step is to write metric for gauging the performance of the algorithm. Write a function to calculate the 'RMS' error given (y_predict, y_truth) to gauge regression and another function to evaluate accuracy of classification. <br>\n",
        "In addition, for classification, we will also use confusion matrix.\n",
        "\n",
        "Below is an example you can use. Feel free to write you own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QIJJVBlPYGc"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def rms(x, y, scale1=None, scale2=None):\n",
        "    '''Calculate the rms error given the truth and the prediction\n",
        "    '''\n",
        "    mask = np.isfinite(x[:]) & np.isfinite(y[:])\n",
        "    if scale1 is not None:\n",
        "        x= scale1.inverse_transform(x)\n",
        "    if scale2 is not None:\n",
        "        y = scale2.inverse_transform(y)\n",
        "    return  np.sqrt(np.mean((x[mask] - y[mask]) ** 2))\n",
        "\n",
        "def acc(x, y):\n",
        "    '''Calculate the accuracy given the truth and the prediction\n",
        "    '''\n",
        "    mask = np.isfinite(x[:]) & np.isfinite(y[:])\n",
        "    return  (x == y).sum()/x.size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFBs4X-SPYGc"
      },
      "source": [
        "#### Hyperparameter method\n",
        "\n",
        "Now, we will be varying hyperparameters to get the best model and build some intuition. There are various ways to do this and we will use Grid Search methodology (as you did in Problem 1 and 2) which simply tries all the combinations along with some cross-validation scheme. For most part, we will use 4-fold cross validation. <br>\n",
        "Sklearn provides GridSearchCV functionality for this purpose. <br>\n",
        "\n",
        "Its recommended to spend some time to go through output format of GridSearchCV and write some utility functions to make the recurring plots for every parameter. <br>\n",
        "Grid Search returns a dictionary with self explanatory keys for the most part. Mostly, the keys correspond to (masked) numpy arrays of size = #(all possible combination of parameters). The value of individual parameter in every combination is given in arrays with keys starting from 'param_\\*' and this should help you to match the combination with the corresponding scores. <br>\n",
        "For masked arrays, you can access the data values by using \\*.data\n",
        "<br>\n",
        "<br>\n",
        "*Do not overwrite these grid search-ed variables (and not only their result) since we will compare all the models together in the end*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceyLUKpxPYGc"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvvxRwqbPYGc"
      },
      "source": [
        "### <span style=\"color:red\"> k Nearest Neighbors\n",
        "\n",
        "For regression, let us play with grid search using knn to tune hyperparmeters. (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) Consider the following 3 hyperparameters - \n",
        "- Number of neighbors ([2, 3, 5, 10, 15, 20, 25, 50, 100])\n",
        "- Weights of leaves (Uniform or Inverse Distance weighing)\n",
        "- Distance metric (Eucledian or Manhattan distance - parameter 'p')\n",
        "\n",
        "<span style=\"color:blue\"> <i> 1. Do a grid search on these parameters. List the combination of hyperparameters you tried and evaluate the accuracy (mean test score) and its standard deviation. Which gives the highest accuracy value? </i></span> <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bghjHVZyPYGc"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o1Bm93xPYGc"
      },
      "source": [
        "Hint: (Read the documentations carefully for more detail.)\n",
        "\n",
        "First, define the hyperparameters: parameters = {'n_neighbors':[2, 3, 5, 10, 15, 20, 25, 50, 100], 'weights':['uniform', 'distance'], 'p':[1, 2]}\n",
        "\n",
        "Specify the algorithm you want to use: e.g. knnr = KNeighborsRegressor() \n",
        "\n",
        "Then, Do a grid search on these parameters using 4 fold cross validation: gcknn = GridSearchCV(knnr, parameters, cv=4)\n",
        "\n",
        "Do the fit: gcknn.fit(*scaled_training_data) \n",
        "\n",
        "(Let \"scaled_training_data\" be the training data where \"scaled_training_data = (\"train X data\", \"train Y data\")\"\n",
        "\n",
        "Get results: $results = gcknn.cv_results_$\n",
        "\n",
        "$cv_results_$ has the following dictionaries: \"rank_test_score,\" \"mean_test_score,\" \"std_test_score,\" and \"params\" (See http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) \n",
        "\n",
        "Then, you can evaluate the models based on \"rank_test_score\" and print out their \"params,\" along with their \"mean_test_score\" and \"std_test_score\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYdcAIORPYGc"
      },
      "source": [
        "parameters = {'n_neighbors':[2, 3, 5, 10, 15, 20, 25, 50, 100], 'weights':['uniform', 'distance'], 'p':[1, 2]}\n",
        "\n",
        "knnr = KNeighborsRegressor()\n",
        "gcknn = GridSearchCV(knnr, parameters, cv=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctMa8A_CA7GB"
      },
      "source": [
        "gcknn.fit(sc_train_reg[0],sc_train_reg[1])\n",
        "results = gcknn.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9kZgfdACWyY"
      },
      "source": [
        "test_score = results['rank_test_score']\n",
        "mean_score = results['mean_test_score']\n",
        "std_score = results['std_test_score']\n",
        "params = results['params']\n",
        "\n",
        "ranks = np.argsort(test_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDEoD5kTDw9G"
      },
      "source": [
        "for i,rank in enumerate(ranks):\n",
        "    print(f'The parameters {params[rank]} ranked {i+1} with a mean score of {mean_score[rank]} and std {std_score[rank]}.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv_aRmMlPYGc"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 2. Also print out fitting and scoring times for all hyperparameter combinations. </i></span> <br>\n",
        "\n",
        "*Plot timings for fitting and scoring*\n",
        "\n",
        "Hint: Assume that you got results from: $results = gcknn.cv_results_$\n",
        "\n",
        "Then, get the scoring time: results['mean_score_time']\n",
        "\n",
        "and the fitting time: results['mean_fit_time']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZVmY_NwPYGc"
      },
      "source": [
        "score_time = results['mean_score_time']\n",
        "fit_time = results['mean_fit_time']\n",
        "\n",
        "for i in range(len(params)):\n",
        "    print(f'For parameters {params[i]}, the scoring time was {score_time[i]}s and the fitting time was {fit_time[i]}s.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSamrUyrGI-n"
      },
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "plt.plot(range(len(score_time)),score_time)\n",
        "\n",
        "plt.title('Scoring time', fontsize=16)\n",
        "plt.ylabel('Time (s)', fontsize=12)\n",
        "plt.xlabel('Model label', fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piAZY2DaG-rA"
      },
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "plt.plot(range(len(fit_time)),fit_time)\n",
        "\n",
        "plt.title('Fit time', fontsize=16)\n",
        "plt.ylabel('Time (s)', fontsize=12)\n",
        "plt.xlabel('Model label', fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXuTfAJ7PYGc"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 3. Based on the results you obtained in Part 1 and 2, answer the following questions </i></span> <br>\n",
        "\n",
        "- Is it always better to use more neighbors?\n",
        "- Is it better to weigh the leaves, if yes, which distnace metric performs better?\n",
        "- GridCV returns fitting and scoring time for every combination. You will find that scoring time is higher than training time. Why do you think is that the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHNDUm7YHKnW"
      },
      "source": [
        "No, we see that the optimal number of neighbors is around 15-25. This makes sense, because too many neighbors leads to overfitting uncorrelated features. We indeed see that weighing the leaves produces better results. The best results were obtained using the 1-norm, but there were results with the 2-norm that were very close. The scoring time is higher than the training time because it is faster to fit data that the model has \"seen,\" rather than data it has not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUXVbkYcPYGc"
      },
      "source": [
        "##### Answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdm_hiaFPYGc"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 4. Which parameters seem to affect the performance most? To better answer this question, make plots of the mean test score for each hyperparameter. </i></span> <br>\n",
        "\n",
        "Hint:\n",
        "Suppose you have two types of hyperparameters: A and B.\n",
        "Let A = [1, 2] and B = [1, 2, 4, 7, 10].\n",
        "\n",
        "Then, you have 20 different combination of hyperparameters.\n",
        "\n",
        "Let A = 1. Then, you can try (A,B) = (1,1), (1,2), (1,4), (1,7), (1,10)\n",
        "Suppose that the mean score you got for the above combination is [0.7, 0.72, 0.75, 0.77, 0.8].\n",
        "Similarly, for A = 2, you tried (A,B) = (2,1), (2,2), (2,4), (2,7), (2,10) and obtaind the mean score of [0.8, 0.82, 0.85, 0.87, 0.9].\n",
        "\n",
        "To better see how changing the value of paramter A affects the performance, you can make the following plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LvcpuD_PYGc"
      },
      "source": [
        "A_1 = [0.7, 0.72, 0.75, 0.77, 0.8]\n",
        "A_2 = [0.8, 0.82, 0.85, 0.87, 0.9]\n",
        "\n",
        "plt.plot(A_1, label = \"A=1\")\n",
        "plt.plot(A_2, label = \"A=2\")\n",
        "plt.ylabel(\"mean test score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owJcc3OPPYGc"
      },
      "source": [
        "This is the plot of the mean test score for A marginalizing over B.\n",
        "\n",
        "Similarly, make a plot of the mean test score for each kNN hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BI5lYZxPYGd"
      },
      "source": [
        "plt.plot(mean_score[0::2], label = \"Weight: uniform\")\n",
        "plt.plot(mean_score[1::2], label = \"Weight: distance\")\n",
        "\n",
        "plt.title('Weighted vs. unweighted leaves',fontsize=14)\n",
        "plt.ylabel(\"mean test score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0CIGENqS1bZ"
      },
      "source": [
        "P_1 = []\n",
        "P_2 = []\n",
        "\n",
        "for i in range(0,len(mean_score),4):\n",
        "    P_1.append(mean_score[i])\n",
        "    P_1.append(mean_score[i+1])\n",
        "    P_2.append(mean_score[i+2])\n",
        "    P_2.append(mean_score[i+3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcuUcs-dPJX7"
      },
      "source": [
        "plt.plot(P_1, label = \"Weight: uniform\")\n",
        "plt.plot(P_2, label = \"Weight: distance\")\n",
        "\n",
        "plt.title('1-norm vs 2-norm',fontsize=14)\n",
        "plt.ylabel(\"mean test score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP67FvSuXQ56"
      },
      "source": [
        "Nn = np.zeros((len(parameters['n_neighbors']), 4))\n",
        "\n",
        "for i in range(0,len(mean_score),4):\n",
        "    j = int(i/4)\n",
        "    Nn[j,:] = mean_score[i:i+4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-RSrTmCZQA6"
      },
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "for i,n in enumerate(parameters['n_neighbors']):\n",
        "   plt.plot(range(4),Nn[i,:], label=f'Number of neighbors: {n}')\n",
        "\n",
        "plt.title('Number of neighbors',fontsize=14)\n",
        "plt.ylabel(\"mean test score\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q28LRsf4bBuI"
      },
      "source": [
        "We see that the number of neighbors and the choice of weighted leaves have the most effect, while the choice of norm is relatively minor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSnh6wt6PYGd"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 5. You have determined the best combination of hyperparameters and CV schemes. Predict the test y data using the GridSearchCV method. Use the \"rms\" metric function we defined earlier and calculate the rms error on the test data.  </i></span> <br>\n",
        "\n",
        "Hint: To determine the rms error, you need:\n",
        "\n",
        "Truth: given from data (test_data[1]) <br>\n",
        "Prediction: gridsearch.predict(test_data[0])\n",
        "(https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDuK9tMqPYGd"
      },
      "source": [
        "predict = gcknn.predict(test_reg[0])\n",
        "\n",
        "rms_error = rms(predict, test_reg[1])\n",
        "\n",
        "print(f'The rms error is {rms_error}.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBwGzd9qPYGd"
      },
      "source": [
        "#### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWn8LOg_PYGd"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHYBj8RBPYGd"
      },
      "source": [
        "Here we will look at 4 different type of cross-validation schemes - \n",
        "- Kfold\n",
        "- Stratified Kfold\n",
        "- Shuffle Split\n",
        "- Stratified Shuffle Split\n",
        "\n",
        "<span style=\"color:blue\"> <i> 6. Assuming the list of hyperparameters from Part 1, do 4 different grid searches. From Part 1, take top 5 combination of hyperparameters which gives you the highest accuracy value. Rank the performance of CV schemes for each combination.  </i></span> <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CaNTZbsPYGd"
      },
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold, ShuffleSplit, StratifiedShuffleSplit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk-X9ZWCPYGd"
      },
      "source": [
        "parameters = {'n_neighbors':[2, 3, 5, 10, 15, 20, 25, 50, 100], 'weights':['uniform', 'distance'], 'p':[1, 2]}\n",
        "knnc = KNeighborsClassifier()\n",
        "\n",
        "#Grid Search\n",
        "gc = GridSearchCV(knnc, parameters, cv=KFold(4, random_state=100))\n",
        "#Do the fit\n",
        "gc.fit(sc_train_cla[0],sc_train_cla[1])\n",
        "gc_results = gc.cv_results_\n",
        "\n",
        "gc2 = GridSearchCV(knnc, parameters, cv=StratifiedKFold(4, random_state = 100))\n",
        "#Do the fit\n",
        "gc2.fit(sc_train_cla[0],sc_train_cla[1])\n",
        "gc2_results = gc2.cv_results_\n",
        "\n",
        "gc3 = GridSearchCV(knnc, parameters, cv=ShuffleSplit(4, 0.1, random_state = 100))\n",
        "#Do the fit\n",
        "gc3.fit(sc_train_cla[0],sc_train_cla[1])\n",
        "gc3_results = gc3.cv_results_\n",
        "\n",
        "gc4 = GridSearchCV(knnc, parameters, cv=StratifiedShuffleSplit(4, 0.1, random_state = 100))\n",
        "#Do the fit\n",
        "gc4.fit(sc_train_cla[0],sc_train_cla[1])\n",
        "gc4_results = gc4.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGzFcOMo3lDF"
      },
      "source": [
        "gcs = (gc_results, gc2_results, gc3_results, gc4_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICwQCOoB0RFj"
      },
      "source": [
        "# getting the top 5 results for each CV scheme\n",
        "cvs = ['KFold', 'StratifiedKFold', 'ShuffleSplit', 'StratifiedShuffleSplit']\n",
        "\n",
        "for i in range(4):\n",
        "    score_rank = np.argsort(gcs[i]['rank_test_score'])[:5]\n",
        "    print(f'\\nFor the CV scheme {cvs[i]}:')\n",
        "\n",
        "    for j,rank in enumerate(score_rank):\n",
        "        p = gcs[i]['params'][int(rank)]\n",
        "        ms = gcs[i]['mean_test_score'][int(rank)]\n",
        "        st = gcs[i]['std_test_score'][int(rank)]\n",
        "        print(f'The parameters {p} ranked {j+1} with a mean score of {ms} and std {st}.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "590MNIhUmrZQ"
      },
      "source": [
        "# getting the results for the top 5 combinations from Part 1\n",
        "top_5 = ranks[0:5]\n",
        "\n",
        "gc_top_5 = np.empty((5,2,4))\n",
        "p_rank = np.empty((5,4))\n",
        "\n",
        "for i, ind in enumerate(top_5):\n",
        "    gc_top_5[i,0,:] = [gc_results['mean_test_score'][ind],gc2_results['mean_test_score'][ind],gc3_results['mean_test_score'][ind],gc4_results['mean_test_score'][ind]]\n",
        "    gc_top_5[i,1,:] = [gc_results['std_test_score'][ind],gc2_results['std_test_score'][ind],gc3_results['std_test_score'][ind],gc4_results['std_test_score'][ind]]\n",
        "\n",
        "    rank = np.flip(np.argsort(gc_top_5[i,0,:]))\n",
        "    gc_top_5[i,0,:] = gc_top_5[i,0,:][rank]\n",
        "    gc_top_5[i,1,:] = gc_top_5[i,0,:][rank]\n",
        "    p_rank[i,:] = rank\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUE5wC8xFL4O"
      },
      "source": [
        "for i in range(5):\n",
        "    p = gc_results['params'][int(top_5[i])]\n",
        "    print(f'\\nFor parameters {p}, the ranked performance of the CV schemes was')\n",
        "\n",
        "    for j in range(4):\n",
        "        q = int(p_rank[i,j])\n",
        "        print(f'{cvs[q]} with accuracy {gc_top_5[i,0,j]} and std {gc_top_5[i,1,j]}.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-UtbsQRPYGd"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 7. Answer the following questions: </i></span> <br>\n",
        "\n",
        "- Are the conclusions different for any parameter from the regression case?\n",
        "- Does the mean accuracy change for different CV scheme?\n",
        "- Does the standard deviation in mean accuracy change?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaYIAMGVPYGd"
      },
      "source": [
        "##### Answer:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u8TI2TCxr1t"
      },
      "source": [
        "We see that the conclusions are slightly different in the classification case versus the regression case. However, they still were consistent in the sense that weighted leaves and an intermediate 10-50 neurons had the best accuracy and largest impact on the score. Similarly, the 1-norm was preferred, but the difference between the 1- and 2-norms was very small.\n",
        "\n",
        "The mean accuracy does  change for different CV schemes, with the largest change on the order of .03.\n",
        "\n",
        "The standard deviation changed slightly, on the order of .001, but mostly smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5ashBa3PYGd"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 8. Using the best combination of hyperparameters and CV schemes you have found, compute the confusion matrix (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and evaluate the accuracy.  </i></span> <br>\n",
        "\n",
        "Hint: To get a confusion matrix, you need both truth (available from data) and prediction (can be computed using .predict function from GridSearchCV (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDgd-7m9PYGd"
      },
      "source": [
        "print('The best combination of parameters and CV scheme is {n_neighbors=25,weights=distance,p=1,cv=ShuffleSplit(4, 0.1, random_state = 100)}.')\n",
        "\n",
        "prediction = gc3.predict(sc_test_cla[0])\n",
        "\n",
        "print('The confusion matrix with these parameters is')\n",
        "confusion_matrix(sc_test_cla[1],prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1eLCOB6PYGe"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "J7uORJaGPYGe"
      },
      "source": [
        "!if [ -d SIG_GIS ]; then rm -Rf SIG_GIS; fi\n",
        "!git clone https://github.com/biweidai/SIG_GIS.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "JK3qxAfj0Bsf"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "sys.path.append('./SIG_GIS/')\n",
        "from GIS import GIS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4Rrt2ZUH4ov"
      },
      "source": [
        "#### Problem 4 - Generative model\n",
        "\n",
        "In the 3 problems above we worked on classification and regression. For those tasks we are given the labels and features of some data, and we try to predict the labels or features of the new data. This is supervised learning. Unsupervised learning is another important field in Machine Learning. In contrast to supervised learning that usually makes use of human-labeled data, unsupervised learning works with datasets with no pre-existing labels and with minimum human supervision (https://en.wikipedia.org/wiki/Unsupervised_learning). In HW 2 we have used UMAP to reduce the dimensionality of MNIST dataset, and then used HDBSCAN to do cluster analysis on the low dimension representation of MNIST. In HW 3 we used Principle Component Analysis (PCA) to model the Quasar spectrum and MNIST. In HW 4 we used Independent Component Analysis (ICA) to identify independent sources in mixed signals. These are all examples of unsupervised learning.\n",
        "\n",
        "#### Normalizing Flow\n",
        "\n",
        "In this problem we will train some generative models. Generative model is an important topic in unsupervised learning. It tries to learn the underlying probability density of the given data (either explicitly or implicitly) so as to generate new data. Some popular models include autoregressive models, Normalizing Flows (NF), Variational Auto-Encoders (VAE), and Generative Adverserial Networks (GAN). In HW 7 we have used a pretrained VAE to reconstruct MNIST images from incomplete and noisy data. In this problem we will focus on NFs. This exercise is based on https://arxiv.org/abs/2007.00674 .\n",
        "\n",
        "A NF maps the data $x$ to latent variables $z$ through a sequence of invertible transformations $f = f_1 \\circ f_2 \\circ ... \\circ f_n$, such that $z = f(x)$ and $z$ is mapped to a base distribution $\\pi(z)$, which is normally chosen to be a standard Gaussian distribution. The probability density of data $x$ can be evaluated using the change of variables formula:\n",
        "\n",
        "$$p(x) = \\pi(f(x)) |\\det \\left(\\frac{\\partial f(x)}{\\partial x}\\right)| = \\pi(f(x)) \\prod_{l=1}^n |\\det \\left(\\frac{\\partial f_l(x)}{\\partial x}\\right)| .$$\n",
        "\n",
        "In order to efficiently evaluate the density, the determinant of the Jacobian of each transformation $\\det (\\frac{\\partial f_l(x)}{\\partial x})$ must be easy to compute. \n",
        "\n",
        "#### Pytorch\n",
        "\n",
        "In HW 7 and this project we have used the Tensorflow and Keras framework. PyTorch is another machine learning framework that becomes very popular in recent years. Compared to Tensorflow, PyTorch is very similar to numpy and very pythonic, and therefore is easy to learn if you are already familiar with Python. While Tensorflow is currently the platform of choice in industry, PyTorch is more popular in the research community. In this problem we will use PyTorch to help you become familiar with this platform. Here is a great tutorial on PyTorch basics: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html . We encourage you to read at least the first topic of the tutorial (What is PyTorch?). We will also give you some examples on PyTorch in the following questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUrvfMq-gB2O"
      },
      "source": [
        "We first look at a simple 1D example. Note that in 1D one has many different ways to estimate the data density, e.g., Kernel Density Estimation (KDE), Gaussian Mixture Model (GMM), histogram, etc.(https://scikit-learn.org/stable/modules/density.html). One does not have to use NF. We give you this 1D example just to show you how NF works. High dimensional problem is where NFs really outperform those methods, and we will look at high dimensional problems later. \n",
        "\n",
        "<span style=\"color:blue\"> <i> 1. Firstly, load in the data and make a histogram plot of the pdf of data. Set the number of bins to 50. Make sure your histogram plot is normalized.</i></span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "16FrSFCAH16P"
      },
      "source": [
        "data = np.load('/content/drive/My Drive/P188_288/P188_288_Project3/data_ThreeGaussian.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ob_pojxjPYGe"
      },
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "plt.hist(data,bins=50,density=True)\n",
        "\n",
        "plt.title('Data')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBBXwN_cvCrL"
      },
      "source": [
        "Now let's use NF to model the data pdf. Assuming that the true data distribution is $p(x)$, our goal is to find an invertible transformation $f$ to map the data $x$ to some latent variables $z=f(x)$ such that $z$ follows a standard Gaussian distribution. For 1D, the solution $f(x)$ is easy to find:\n",
        "$$f(x)=G^{-1}(F(x)) ,$$\n",
        "where $F$ is the Cumulative Distribution Function (CDF) of data distribution, and $G$ is the CDF of standard Gaussian. Then the pdf of data $p(x)$ is given by the change of variable formula\n",
        "$$p(x)=N(f(x))|\\frac{df(x)}{dx}| ,$$\n",
        "where $N$ is the pdf of standard Gaussian. \n",
        "\n",
        "In this problem we will approximate $f(x)$ using piecewise rational quadratic spline functions, which are monotonic, differentiable and analytically invertible. Here we provide you an example of using piecewise rational quadratic spline functions to approximate a simple function $y=x^3$, and then calculate the function derivative and invert the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "-QoRJjtS4Soc"
      },
      "source": [
        "from RQspline import RQspline\n",
        "\n",
        "def RQspline_interp(x, y):\n",
        "\n",
        "    #x and y are torch 1d tensors and they must be strictly monotonic\n",
        "\n",
        "    assert x.ndim == 1 and y.ndim == 1\n",
        "    assert len(x) == len(y)\n",
        "\n",
        "    ndim = 1\n",
        "    nknot = len(x)\n",
        "    spline = RQspline(ndim, nknot).requires_grad_(False)\n",
        "\n",
        "    dy = y[1:] - y[:-1]\n",
        "    dx = x[1:] - x[:-1]\n",
        "    assert (dx>0).all() and (dy>0).all()\n",
        "\n",
        "    h = dx\n",
        "    s = dy / dx\n",
        "    deriv = torch.zeros_like(x)\n",
        "    deriv[1:-1] = (s[:-1]*h[1:] + s[1:]*h[:-1]) / (h[1:] + h[:-1])\n",
        "    deriv[0] = dy[0] / dx[0]\n",
        "    deriv[-1] = dy[-1] / dx[-1]\n",
        "\n",
        "    spline.set_param(x.reshape(1,-1), y.reshape(1,-1), deriv.reshape(1,-1))\n",
        "    return spline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "lzSmQ4dxvtZM"
      },
      "source": [
        "# create an array of x and y\n",
        "x = np.linspace(-3, 3, 10, endpoint=True)\n",
        "y = x**3\n",
        "\n",
        "# transform x and y to PyTorch tensors\n",
        "x = torch.tensor(x)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "# define piecewise ratianal quadratic spline functions\n",
        "spline = RQspline_interp(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDK6A8296bug"
      },
      "source": [
        "#plot original function\n",
        "x_plot = np.linspace(-3, 3, 100, endpoint=True)\n",
        "y_plot = x_plot**3\n",
        "plt.plot(x_plot, y_plot, label='$y=x^3$')\n",
        "\n",
        "#Create PyTorch tensor x. The data type must be float (not double), and the shape must be (n, 1)\n",
        "x_plot_tensor = torch.tensor(x_plot).float().reshape(-1,1)\n",
        "\n",
        "#Call the spline function. It returns y and log(dy/dx)\n",
        "y_plot_tensor, log_deriv = spline(x_plot_tensor)\n",
        "plt.plot(x_plot_tensor.numpy(), y_plot_tensor.numpy(), label='interpolation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNSwIuqd_L7u"
      },
      "source": [
        "The derivative from interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YBnJYCb88Rg"
      },
      "source": [
        "#plot dy/dx\n",
        "deriv = 3*x_plot**2\n",
        "plt.plot(x_plot, deriv, label='derivative of $y=x^3$')\n",
        "\n",
        "#plot dy/dx of interpolation\n",
        "plt.plot(x_plot_tensor.numpy(), torch.exp(log_deriv).numpy(), label='derivative of interpolation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QkozxMj-iRL"
      },
      "source": [
        "The derivatives are not accurate around the boundaries due to boundary effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4ojVIF9_srH"
      },
      "source": [
        "Invert the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xErqoleJ91dd"
      },
      "source": [
        "# invert the original function\n",
        "y_plot = x_plot**(1/3)\n",
        "plt.plot(x_plot, y_plot, label='$y=x^{1/3}$')\n",
        "\n",
        "#Invert the spline function. \n",
        "y_plot_tensor = spline.inverse(x_plot_tensor)[0]\n",
        "plt.plot(x_plot_tensor.numpy(), y_plot_tensor.numpy(), label='inversion of spline functions')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXukGJeWy1Hk"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 2. Estimate the transformation $f(x)$ that transforms the data to a standard Gaussian distribution. Follow the above example, use RQspline_interp function to approximate $f(x)$. Plot $f(x)$, $\\frac{df(x)}{dx}$, and $f^{-1}(x)$. Set your x to be within (-5, 5) when making the plot. </i></span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vay-9jNjdQNw"
      },
      "source": [
        "import scipy.stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7x5nJQOKZxR_"
      },
      "source": [
        "# estimating the Gaussian using the percentiles of the data\n",
        "data_q = np.empty(98)\n",
        "\n",
        "for i in range(98):\n",
        "    data_q[i] = np.percentile(data,i+1)\n",
        "\n",
        "\n",
        "gauss_q = np.empty(98)\n",
        "for i in range(98):\n",
        "    gauss_q[i] = scipy.stats.norm.ppf((i+1)/100,loc=0,scale=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hOq2LuYXDtAy"
      },
      "source": [
        "x = torch.tensor(data_q)\n",
        "y = torch.tensor(gauss_q)\n",
        "\n",
        "spline = RQspline_interp(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB4CsB1QfpCa"
      },
      "source": [
        "x_plot = np.linspace(-5, 5, 100, endpoint=True)\n",
        "\n",
        "x_plot_tensor = torch.tensor(x_plot).float().reshape(-1,1)\n",
        "\n",
        "y_plot_tensor, log_deriv = spline(x_plot_tensor)\n",
        "plt.plot(x_plot_tensor.numpy(), y_plot_tensor.numpy())\n",
        "\n",
        "plt.title('Interpolation of percentile function')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQNvppzok2Lt"
      },
      "source": [
        "deriv = torch.exp(log_deriv).numpy()\n",
        "\n",
        "plt.plot(x_plot_tensor.numpy(), deriv)\n",
        "\n",
        "plt.title('Derivative of interpolated percentile function')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go_MsOCxl8iV"
      },
      "source": [
        "y_plot_tensor = spline.inverse(x_plot_tensor)[0]\n",
        "plt.plot(x_plot_tensor.numpy(), y_plot_tensor.numpy())\n",
        "plt.title('Inverted spline')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JASbinKCDtwC"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 3. Calculate latent variable $z=f(x)$. Make a histogram of pdf of $z$. Does it follow a Gaussian distribution? </i></span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "UgJIAPKKPYGg"
      },
      "source": [
        "data_tensor = torch.tensor(data).float().reshape(-1,1)\n",
        "\n",
        "z_tensor, z_logderiv = spline(data_tensor)\n",
        "\n",
        "z = z_tensor.numpy()\n",
        "dz = torch.exp(z_logderiv).numpy()\n",
        "\n",
        "plt.hist(z, bins = 20, density=True)\n",
        "plt.title('Spline histogram')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print('Wow, that looks pretty Gaussian!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc1sGRJjEMRk"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 4. Now let's estimate the pdf of the data using the \n",
        "change of variable formula $p(x)=N(f(x))|\\frac{df(x)}{dx}|$. Make a plot of $p(x)$. How does it compare with the histogram in Q1?</i></span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hV1ZZEVLPYGg"
      },
      "source": [
        "Nf = scipy.stats.norm.pdf(z)\n",
        "p = Nf*np.abs(dz)\n",
        "\n",
        "plt.figure(figsize = (8,6))\n",
        "\n",
        "plt.plot(data,p,'.')\n",
        "plt.title('Approximate pdf of data')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSfM3o3EajS6"
      },
      "source": [
        "The pdf is noisy. One could apply regularization when modeling to get a smoother pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S30mrHSE5xr"
      },
      "source": [
        "NFs allow us to sample from $p(x)$ to create new data. The idea is to firstly sample latent variable $z$ from $N(0,I)$ using torch.randn(nsample, ndim), and then transform $z$ to $x$ with inverse transform $f^{-1}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlhBjy7AEy21"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 5. Sample from $p(x)$ using $x=f^{-1}(z)$. Get 10000 samples. Make a histogram of the pdf of samples. Does it agree with Q1 and Q4? </i></span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IvlonqjvPYGg"
      },
      "source": [
        "z_lat = torch.randn(10000, 1)\n",
        "\n",
        "x_z = spline.inverse(z_lat)[0]\n",
        "\n",
        "plt.figure(figsize = (8,6))\n",
        "\n",
        "plt.hist(x_z.numpy(), bins = 20, density=True)\n",
        "plt.title('NF random sampled histogram')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu-2odG0GYYE"
      },
      "source": [
        "We have built a NF model in 1D. Now let's look at multidimensional problems. Similar to 1D, our goal is to construct an invertile transformation $f$ that transforms data $x$ to latent variables $z$ that follow standard Gaussian distribution. There are multiple ways to build such transformations. For example, lots of NF models parametrize $f$ with neural networks, and the parameters are trained with maximum likelihood. Here we use a different approach and solve this problem iteratively. In each iteration, we view the N-dimensional problem as N 1-dimensional problem, and use the transformation we developed in 1D to transform these N 1-dimensional variables to standard Gaussian. For example, say your dataset is 2-dimensional $x=\\{x_1, x_2\\}$. We apply transformations $z_1=f_1(x_1)$ and $z_2=f_2(x_2)$ such that $z_1$ and $z_2$ follow Gaussian distribution separately. However, $z_1$ and $z_2$ combined are not Gaussian, so in the next iteration, we apply a rotation to $\\{z_1,z_2\\}$, and then redo the 1D Gaussianization on all of the variables. We keep iterating until the pdf of variables converge to Gaussian. The rotation matrix in each iteration will be determined by ICA (see HW 4) to maximizes the independence (non-Gaussianity) of different axes. This model is called GIS (Gaussianizing Iterative Slicing). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwR8iWytkEep"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 6. Follow the instructions below and use GIS to model the 2D dataset. Transform the training and test data to latent space, and make scatter plots. Do they follow Gaussian distribution? Print the mean logp of training, validation and test data. Plot the logp field using imshow. </i></span>\n",
        "\n",
        "1.   Divide the data into training (80%), validation (10%) and test (10%) set. Transform them into Pytorch tensors. \n",
        "2.   Apply the GIS model to the data: model=GIS(data_train, data_validate)\n",
        "3.   Transform the data to latent space: latent = model.forward(data)[0]. Make scatter plots of the latent training data and latent test data.\n",
        "4.   Evaluate the logp of training, validation and test data using logp = model.evaluate_density(data). Print the mean logp of each set.\n",
        "5.   Create 2-dimensional grid points x. Evaluate the logp on the grid points. Plot the logp field using plt.imshow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Vu5KW4gxdbPA"
      },
      "source": [
        "data = np.load('/content/drive/My Drive/P188_288/P188_288_Project3/data_TwoSpiral.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mX2oD-nTxnR",
        "scrolled": true
      },
      "source": [
        "plt.scatter(data[:,0], data[:,1], s=5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXi7se6OMeHY"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3iF-C8qzPYGg"
      },
      "source": [
        "X_train, X_test = train_test_split(data, train_size = 0.8,random_state=1)\n",
        "X_test, X_valid = train_test_split(X_test, train_size = 0.5,random_state=1)\n",
        "\n",
        "train = torch.tensor(X_train).float()\n",
        "test = torch.tensor(X_test).float()\n",
        "valid = torch.tensor(X_valid).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE1VvdWwyzoz"
      },
      "source": [
        "model = GIS(train, valid)\n",
        "\n",
        "latent_train = model.forward(train)[0]\n",
        "latent_test = model.forward(test)[0]\n",
        "latent_valid = model.forward(valid)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLtrMHUC2bUN"
      },
      "source": [
        "plt.scatter(latent_train[:,0],latent_train[:,1], s=1)\n",
        "plt.title('Latent training data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5u7ncuW3v6s"
      },
      "source": [
        "plt.scatter(latent_test[:,0],latent_test[:,1], s=5)\n",
        "plt.title('Latent test data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmAkFs5v39LG"
      },
      "source": [
        "logp_train = model.evaluate_density(train)\n",
        "logp_test = model.evaluate_density(test)\n",
        "logp_valid = model.evaluate_density(valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Zs11_v98PYGg"
      },
      "source": [
        "print(f'Mean logp for training data: {np.mean(logp_train.numpy())}.')\n",
        "print(f'Mean logp for validation data: {np.mean(logp_test.numpy())}.')\n",
        "print(f'Mean logp for test data: {np.mean(logp_test.numpy())}.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3spORPH2PYGg"
      },
      "source": [
        "x = np.linspace(-10, 10, 20)\n",
        "y = np.linspace(-10,10,20)\n",
        "\n",
        "x_c, y_c = np.meshgrid(x,y)\n",
        "\n",
        "xy = np.array([x_c,y_c])\n",
        "xy = torch.tensor(xy).float()\n",
        "\n",
        "logp_grid = np.empty((20,20))\n",
        "\n",
        "# was having issues vectorizing....\n",
        "for i in range(20):\n",
        "    for j in range(20):\n",
        "        logp_grid[i,j] = model.evaluate_density(xy[:,i,j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3AZiQ8b-hXH"
      },
      "source": [
        "plt.imshow(logp_grid)\n",
        "plt.title('Grid evaluation of logp')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfFYPaxoF-nb"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 7. Create samples by firstly sample latent variables from standard Gaussian using torch.randn(nsample, ndim), and then transform the latent variables to data space using sample = model.inverse(latent)[0]. Make a scatter plot of the samples. Do they look similar to the data? </i></span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2I8CJtxSTuH"
      },
      "source": [
        "latent_samp = torch.randn(10000,2)\n",
        "latent_samp = torch.tensor(latent_samp)\n",
        "\n",
        "sample = model.inverse(latent_samp)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOx-MDpQVvC3"
      },
      "source": [
        "plt.scatter(sample[:,0],sample[:,1], s=1)\n",
        "plt.title('Samples')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iStdOnYp8QDU"
      },
      "source": [
        "We have applied GIS to 1D and 2D toy exampels. Now let's look at a real-world dataset. The HEPMASS dataset (https://archive.ics.uci.edu/ml/datasets/HEPMASS) is one of the standard dataset for testing the performance of density estimation models. This dataset comes from particle collisions in high-energy physics experiments, with data dimensionality 21. Run the below cell to load in HEPMASS dataset. The data has already been preprocessed. The original dataset has 315123 training data. In this question we only use 10000 of them. The validation and test set are kept the same as the original dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ONjv5oef-g1l"
      },
      "source": [
        "data_train = np.load('/content/drive/My Drive/P188_288/P188_288_Project3/HEPMASS_train.npy')\n",
        "data_validate = np.load('/content/drive/My Drive/P188_288/P188_288_Project3/HEPMASS_validate.npy')\n",
        "data_test = np.load('/content/drive/My Drive/P188_288/P188_288_Project3/HEPMASS_test.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oaAUZWyGw0p"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 8. Apply GIS to the HEPMASS dataset, and print the mean logp of training, validation and test data. Model the same dataset with Kernel Density Estimation (KDE) using sklearn.neighbors.KernelDensity. Find the optimal bandwidth of KDE kernels that maximizes the mean logp of validation set. Print the mean logp of training, validation and test data from KDE. Which method gives you better results? \n",
        "\n",
        "You could use a GPU in this question by going to Edit -> Notebook Settings -> Hardware accelerator -> GPU -> SAVE. If GPU is enabled, the model returned by GIS function will be on GPU. Before calculating mean logp, you should either move the model to CPU by doing model = model.cpu(), or move the data to GPU by doing data = data.cuda(). </i></span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "h0DAzbSICLyu"
      },
      "source": [
        "d_train = torch.tensor(data_train)\n",
        "d_valid = torch.tensor(data_validate)\n",
        "d_test = torch.tensor(data_test)\n",
        "\n",
        "model = GIS(d_train, d_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIHTZoP0PZgh"
      },
      "source": [
        "d_traing = d_train.cuda()\n",
        "d_validg = d_valid.cuda()\n",
        "d_testg = d_test.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iIwvkswT4ZF"
      },
      "source": [
        "logp_train = model.evaluate_density(d_traing)\n",
        "logp_test = model.evaluate_density(d_testg)\n",
        "logp_valid = model.evaluate_density(d_validg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X07H6lskVcEy"
      },
      "source": [
        "logp_trainc = logp_train.cpu()\n",
        "logp_testc = logp_test.cpu()\n",
        "logp_validc = logp_valid.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "j7vUjv-YPYGh"
      },
      "source": [
        "print('GIS:')\n",
        "print(f'Mean logp for training data: {np.mean(logp_trainc.numpy())}.')\n",
        "print(f'Mean logp for validation data: {np.mean(logp_validc.numpy())}.')\n",
        "print(f'Mean logp for test data: {np.mean(logp_testc.numpy())}.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILjYeUH5ZPex"
      },
      "source": [
        "from sklearn.neighbors import KernelDensity\n",
        "from scipy.optimize import fmin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KJCEDtoZtp_"
      },
      "source": [
        "def mlogp_band(bandwidth):\n",
        "    kde = KernelDensity(bandwidth=bandwidth)\n",
        "    kde.fit(data_train)\n",
        "    mean_logp = np.mean(kde.score_samples(data_validate))\n",
        "\n",
        "    return mean_logp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "547KDtvxqkgD"
      },
      "source": [
        "# basic grid search, since it takes a long time to compute the mean logp\n",
        "grid = np.logspace(-1,1,10)\n",
        "\n",
        "mlogp = np.empty(10)\n",
        "\n",
        "for i in range(10):\n",
        "    mlogp[i] = mlogp_band(grid[i])\n",
        "\n",
        "bw = grid[np.argmax(mlogp)]\n",
        "mlogp_valid = np.max(mlogp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcQxO2J8hBQ2"
      },
      "source": [
        "kde = KernelDensity(bandwidth=bw)\n",
        "kde.fit(data_train)\n",
        "mlogp_train = np.mean(kde.score_samples(data_train))\n",
        "mlogp_test = np.mean(kde.score_samples(data_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GFbxfULoPYGh"
      },
      "source": [
        "print(f'KDE with optimal bandwidth {bw}:')\n",
        "print(f'Mean logp for training data: {mlogp_train}')\n",
        "print(f'Mean logp for validation data: {mlogp_valid}')\n",
        "print(f'Mean logp for test data: {mlogp_test}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHt0DlKtHoCN"
      },
      "source": [
        "GIS iteratively maps the data to latent variables $z$ which follow Gaussian distribution. The mapping is invertible, so another way to build the NF is to start from latent Gaussian variables $z$, and then iteratively maps them to data. This model learns the mapping in the opposite direction of GIS, so we name it SIG (Sliced Iterative Generator). While GIS is good at density estimation, SIG normally gives samples with better quanlity in high dimensions. In the next question you will train SIG on MNIST dataset and then generate some samples of hand written digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81LIgTEnrrzK"
      },
      "source": [
        "\n",
        "<span style=\"color:blue\"> <i> 9. Follow the instructions below and use SIG to model the MNIST dataset. Plot any 5 of your test samples. Then use the same latent variable and plot the corresponding sample for 0, 10, 20, 50 and 100 iterations. Make sure that you use a GPU in this question by going to Edit -> Notebook Settings -> Hardware accelerator -> GPU -> SAVE. </i></span>\n",
        "\n",
        "1.   Load in data: data_train, data_test = load_data_mnist()\n",
        "2.   Data preprocessing: data = preprocess(data)\n",
        "3.   Define the model: model = SIT(ndim=784).requires_grad_(False)\n",
        "4.   Create 60k Gaussian samples for training: sample = torch.randn(nsample, ndim). Create 100 Gaussian samples for test (sample_test).\n",
        "5.   Apply 100 iterations. To add one iteration, do \n",
        "\n",
        "     model, sample, sample_test = add_one_layer_inverse(model, data_train, sample, n_component, nsample_wT, nsample, sample_test=sample_test, batchsize=batchsize).\n",
        "\n",
        "     Set hyperparameters n_component=56, nsample_wT=60000, nsample=60000.\n",
        "\n",
        "     Set batchsize=10000 to prevent out of memory issue.\n",
        "6.   Plot any 5 of your test samples. You could transform the sample_test to images using: sample_test_image = toimage(sample_test, shape=[28,28]). \n",
        "7.   Generate a random latent variable using torch.randn(nsample, ndim). Plot the corresponding sample for 0, 10, 20, 50 and 100 iterations. For example, to apply 20 iterations on the latent variable, do sample_20iter = model.inverse(latent, end=-20)[0]. Note that you should use the same latent variable for generating these 5 samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "psluw_pb7NaW"
      },
      "source": [
        "from load_data import load_data_mnist\n",
        "from SIT import SIT\n",
        "from SIG import add_one_layer_inverse, preprocess, toimage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rsHzTI0vPYGh"
      },
      "source": [
        "data_train, data_test = load_data_mnist()\n",
        "\n",
        "data_train = preprocess(data_train)\n",
        "data_test = preprocess(data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGuBdxyO4pVi"
      },
      "source": [
        "model0 = SIT(ndim=784).requires_grad_(False)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "sample_0 = torch.randn(60000, 784)\n",
        "sample_test_0 = torch.randn(100,784)\n",
        "\n",
        "sample_n = torch.clone(sample_0)\n",
        "sample_test_n = torch.clone(sample_test_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1H3cIaL4uT8"
      },
      "source": [
        "for i in range(100):\n",
        "    model0, sample_n, sample_test_n = add_one_layer_inverse(model0, data_train, sample_n, n_component=56, nsample_wT=60000, nsample=60000, sample_test=sample_test_n, batchsize=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYvuRqMi3x2u"
      },
      "source": [
        "#save the model for future use\n",
        "torch.save(model0, '/content/drive/My Drive/P188_288/P188_288_Project3/SIG_0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvT2R6BwAiGh"
      },
      "source": [
        "sample_test_image = toimage(sample_test_n, shape=[28,28])\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    plt.imshow(sample_test_image[i,:,:], cmap=plt.cm.gray)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YKKldTiBx3w"
      },
      "source": [
        "latent = torch.randn(1, 784)\n",
        "plot_i = [0,10,20,50,100]\n",
        "\n",
        "\n",
        "for n in plot_i:\n",
        "    latent = model.inverse(latent, end=-n)[0]\n",
        "    latent_im = toimage(latent, shape=[28,28])\n",
        "    plt.imshow(latent_im[0], cmap=plt.cm.gray)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br45WTiHH-1L"
      },
      "source": [
        "The samples should be pretty noisy, but you should be able tell which digits they are. To get better samples, you need to use patch-based modeling, which takes advantage of the fact that neighboring pixels in images have stronger correlations than pixels that are far away. For more details on patch-based modeling, see https://arxiv.org/abs/2007.00674 ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th2MfpoCLF3H"
      },
      "source": [
        "<span style=\"color:blue\"> <i> 10. Follow the instruction below and add 100 more patch-based iterations to SIG (this takes about 1.5 h). Plot the same 5 test samples as Q9 part 6. Do they look better now? </i></span>\n",
        "\n",
        "To add one patch-based iteration, do \n",
        "\n",
        "model, sample, sample_test = add_one_layer_inverse(model, data_train, sample, n_component, nsample_wT, nsample, sample_test=sample_test, batchsize=batchsize, layer_type='patch', shape=[28,28,1], kernel=kernel, shift=shift).\n",
        "\n",
        "For the first 50 iterations, set n_component=28, kernel=[14,14,1], shift=torch.randint(14, (2,)).tolist() (random shift for each iteration)\n",
        "\n",
        "For the next 30 iterations, set  n_component=14, kernel=[7,7,1], shift=torch.randint(7, (2,)).tolist() (random shift for each iteration)\n",
        "\n",
        "For the last 30 iterations, set  n_component=8, kernel=[4,4,1], shift=torch.randint(4, (2,)).tolist() (random shift for each iteration)\n",
        "\n",
        "The other hyperparameters are the same as Q9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "j-wGxBzHPYGh"
      },
      "source": [
        "model = SIT(ndim=784).requires_grad_(False)\n",
        "\n",
        "sample = torch.clone(sample_0)\n",
        "sample_test = torch.clone(sample_test_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVrevw3FJ9od"
      },
      "source": [
        "for i in range(110):\n",
        "    nsample_wT=60000\n",
        "    nsample=60000\n",
        "    batchsize=10000\n",
        "    if i < 50:\n",
        "        n_component=28\n",
        "        kernel=[14,14,1]\n",
        "        shift=torch.randint(14, (2,)).tolist()\n",
        "        model, sample, sample_test = add_one_layer_inverse(model, data_train, sample, n_component=n_component, nsample_wT=nsample_wT, nsample=nsample, sample_test=sample_test, batchsize=batchsize, layer_type='patch', shape=[28,28,1], kernel=kernel, shift=shift)\n",
        "\n",
        "    if (i > 49) and (i < 80):\n",
        "        n_component=14\n",
        "        kernel=[7,7,1]\n",
        "        shift=torch.randint(7, (2,)).tolist()\n",
        "        model, sample, sample_test = add_one_layer_inverse(model, data_train, sample, n_component=n_component, nsample_wT=nsample_wT, nsample=nsample, sample_test=sample_test, batchsize=batchsize, layer_type='patch', shape=[28,28,1], kernel=kernel, shift=shift)\n",
        "\n",
        "    if i > 79:\n",
        "        n_component=8\n",
        "        kernel=[4,4,1]\n",
        "        shift=torch.randint(4, (2,)).tolist()\n",
        "        model, sample, sample_test = add_one_layer_inverse(model, data_train, sample, n_component=n_component, nsample_wT=nsample_wT, nsample=nsample, sample_test=sample_test, batchsize=batchsize, layer_type='patch', shape=[28,28,1], kernel=kernel, shift=shift)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9zDQf0TtJSmt"
      },
      "source": [
        "#save the model for future use\n",
        "#torch.save(model, '/content/drive/My Drive/P188_288/P188_288_Project3/SIG')\n",
        "\n",
        "#load the model\n",
        "model_patch = torch.load('/content/drive/My Drive/P188_288/P188_288_Project3/SIG')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4524w5yPYGh"
      },
      "source": [
        "# applying saved patch model to first 5 random samples\n",
        "for i in range(5):\n",
        "    latent = model_patch.inverse(sample_test_0[i], end=-100)[0]\n",
        "    latent_im = toimage(latent, shape=[28,28])\n",
        "    plt.imshow(latent_im[0], cmap=plt.cm.gray)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUDqpcvyPYGh"
      },
      "source": [
        "***"
      ]
    }
  ]
}